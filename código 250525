
import pomdp_py
from pomdp_py.utils import TreeDebugger
from pomdp_py import Particles
import random
import numpy as np
import sys
import copy
import math
from itertools import product
import json
import logging
from tqdm import tqdm
from math import floor
import time
from pgmpy.models.DiscreteBayesianNetwork import DiscreteBayesianNetwork as BayesianModel
from pgmpy.factors.discrete import TabularCPD
from pgmpy.inference import VariableElimination
import traceback
from pythermalcomfort.models import pmv_ppd_iso
from pomdp_py.algorithms.po_uct import POUCT
from pomdp_py.algorithms.po_uct import RandomRollout


def setup_logging():

    root_logger = logging.getLogger()
    if root_logger.hasHandlers():
        root_logger.handlers.clear()

    logging.basicConfig(
        filename="registro_consola.log",
        filemode="w",
        format="%(asctime)s - %(levelname)s - %(message)s",
        level=logging.DEBUG,
        force=True  
    )

    class RedirigirSalida:
        def write(self, message):
            if message.strip(): 
                logging.info(message.strip())
        def flush(self):
            pass

    sys.stdout = RedirigirSalida()
    sys.stderr = RedirigirSalida()

setup_logging()



class EstadoConfortTermico(pomdp_py.State):
    def __init__(self, ambiente, control, persona, transferencia, pmv=None):
        super().__init__()
        
        if not all(isinstance(param, dict) for param in [ambiente, control, persona, transferencia]):
            raise ValueError("[ERROR] Los parámetros deben ser diccionarios válidos.")

        self.ambiente = ambiente
        self.control = control
        self.persona = persona
        self.transferencia = transferencia
        self.pmv = pmv if pmv is not None else self.calcular_pmv()

    def calcular_pmv(self):
        try:
            # Limitar valores dentro de rangos razonables
            tdb = np.clip(self.ambiente.get("tdb", 25), 10, 30)
            tr = np.clip(self.ambiente.get("Tr", tdb), 10, 40)
            rh = np.clip(self.ambiente.get("HR", self.ambiente.get("Hr", 50)), 20, 90)
            met = np.clip(self.persona.get("M", 1.2), 0.8, 4)
            clo = np.clip(self.persona.get("Icl", 0.8), 0, 2)
            wme = np.clip(self.persona.get("W", 0.0), 0, 0.1)
            vr = np.clip(self.control.get("vr", 0.1), 0, 1)

            logging.info(f"DEBUG: calcular_pmv - tdb: {tdb}, tr: {tr}, rh: {rh}, met: {met}, clo: {clo}, wme: {wme}, vr: {vr}")

            resultado = pmv_ppd_iso(tdb=tdb, tr=tr, vr=vr, rh=rh, met=met, clo=clo, wme=wme)

            # Manejo de posibles errores de acceso al PMV
            try:
                pmv_value = getattr(resultado, "pmv", None)
            except AttributeError:
                logging.error(f"ERROR: pmv_ppd_iso devolvió un objeto inesperado: {resultado}")
                pmv_value = random.uniform(-0.5, 0.5)

            # Validación adicional si pmv_value es NaN o no válido
            if pmv_value is None or (isinstance(pmv_value, float) and np.isnan(pmv_value)):
                logging.error("ERROR: pmv_ppd_iso devolvió NaN, usando valor aleatorio.")
                pmv_value = random.uniform(-0.5, 0.5)

            if abs(pmv_value) < 1e-6:
                logging.info("DEBUG: PMV calculado es casi 0, se ajusta para introducir variabilidad.")
                pmv_value = random.uniform(-0.5, 0.5)

            logging.info(f"DEBUG: Resultado de pmv_ppd_iso: PMV final: {pmv_value}")
            return pmv_value

        except Exception as e:
            logging.error(f"[ERROR] Falló el cálculo de PMV con datos: {self.ambiente}, {self.control}, {self.persona}. Excepción: {e}")
            return random.uniform(-0.5, 0.5)

    @classmethod
    def generar_estado_aleatorio(cls):
        tdb = round(random.uniform(18, 30), 1)
        tr = round(np.clip(tdb + random.gauss(0, 2), 18, 30), 1)
        ambiente = {
            "Hr": round(random.uniform(30, 70), 1),
            "tdb": tdb,
            "Tr": tr,
            "Pa": round(random.uniform(500, 5600), 1)
        }
        control = {
            "Tt": round(random.uniform(16, 33), 1),
            "vr": round(np.clip(random.uniform(0, 2), 0, 2), 1)
        }
        persona = {
            "M": round(np.clip(random.uniform(0.8, 1.8), 0.8, 4.0), 2),
            "W": round(np.clip(random.uniform(0.1, 0.3), 0, 0.3), 2),
            "Icl": round(np.clip(random.uniform(0.5, 1.2), 0.5, 1.2), 2)
        }
        transferencia = {
            "hc": round(np.clip(random.uniform(3.5, 5), 2, 6), 2),
            "Tcl": round(np.clip(random.uniform(22, 28), 5, 30), 2)
        }
        return cls(ambiente=ambiente, control=control, persona=persona, transferencia=transferencia)

    def simular_observacion(self):
        """
        Genera una observación simulada a partir del estado.
        Devuelve un diccionario con las claves 'tdb', 'Tt', 'Vr' y 'Hr'.
        Se agrega logging para inspeccionar el contenido y verificar rangos.
        """
        obs = {
            "tdb": self.ambiente.get("tdb", 25),
            "Tt": self.control.get("Tt", 25),  # Ajusta el valor por defecto según corresponda
            "Vr": self.control.get("vr", 0.1),
            "Hr": self.ambiente.get("Hr", 50)
        }
    
        logging.info(f"[DEBUG] Observación simulada generada: {obs}")
    
        # Verificar rangos esperados
        if not (15 <= obs["tdb"] <= 30):
            logging.warning(f"[DEBUG] tdb ({obs['tdb']}) fuera del rango esperado [15, 30].")
        if not (15 <= obs["Tt"] <= 40):
            logging.warning(f"[DEBUG] Tt ({obs['Tt']}) fuera del rango esperado [15, 40].")
        if not (0 <= obs["Vr"] <= 2.0):
            logging.warning(f"[DEBUG] Vr ({obs['Vr']}) fuera del rango esperado [0, 2.0].")
        if not (20 <= obs["Hr"] <= 90):
            logging.warning(f"[DEBUG] Hr ({obs['Hr']}) fuera del rango esperado [20, 90].")
    
        return obs

    def __repr__(self):
        return (f"EstadoConfortTermico(\n"
                f"  ambiente={self.ambiente},\n"
                f"  control={self.control},\n"
                f"  persona={self.persona},\n"
                f"  transferencia={self.transferencia},\n"
                f"  pmv={self.pmv:.2f}\n)")

    def __eq__(self, other):
        if isinstance(other, EstadoConfortTermico):
            return (self.ambiente == other.ambiente and
                    self.control == other.control and
                    self.persona == other.persona and
                    self.transferencia == other.transferencia and
                    round(self.pmv, 5) == round(other.pmv, 5))
        return False

    def __hash__(self):
        ambiente_hash = tuple(sorted((k, float(v)) for k, v in self.ambiente.items()))
        control_hash = tuple(sorted((k, float(v)) for k, v in self.control.items()))
        persona_hash = tuple(sorted((k, float(v)) for k, v in self.persona.items()))
        transferencia_hash = tuple(sorted((k, float(v)) for k, v in self.transferencia.items()))
        return hash((ambiente_hash, control_hash, persona_hash, transferencia_hash, round(self.pmv, 5)))

# Ejemplo de función auxiliar para convertir observaciones a vector (si se requiere para comparaciones numéricas)
def observacion_a_vector(observacion):
    """
    Convierte una observación a un vector numérico.
    Se espera que la observación sea un diccionario con las claves 'tdb', 'Tr', 'Hr' y 'Pa'.
    """
    orden = ["tdb", "Tt", "Vr", "Hr"]
    return np.array([observacion.get(clave, 0) for clave in orden])



class AgentePersonalizado(pomdp_py.Agent):
    def __init__(self, belief, policy, transition_model, observation_model, reward_model, pmv, nombre=None):
        self.nombre = nombre or "AgenteDesconocido"

        try:
            if not isinstance(belief, pomdp_py.Particles): 
                raise TypeError("[ERROR] 'belief' debe ser una instancia de pomdp_py.Particles.")

            if not isinstance(policy, ModeloDePolitica):
                raise TypeError("[ERROR] 'policy' no es una instancia de ModeloDePolitica.")

            if not all([transition_model, observation_model, reward_model]):
                raise ValueError("[ERROR] Uno o más modelos (transición, observación, recompensa) son None.")

            self.policy = policy
            super().__init__(belief, policy, transition_model, observation_model, reward_model)

            if self.policy is None:
                raise AttributeError("[ERROR] 'policy' no fue asignado correctamente.")

            self.pmv = pmv
            self.rollout_policy = pomdp_py.RandomRollout()
            logging.debug(f"[DEBUG] Tipo de rollout_policy antes de POUCT: {type(self.rollout_policy)}")

            self.planificador = pomdp_py.POUCT(
                max_depth=10,
                num_sims=500,
                discount_factor=0.95,
                rollout_policy=self.rollout_policy  
            )

            logging.info(f"[INFO] AgentePersonalizado inicializado con éxito: {self.nombre}")
        except Exception as e:
            logging.exception("[ERROR] Falló la inicialización de AgentePersonalizado:")
            raise e

    def plan(self):
        
        try:
            accion = super().plan() 
        except Exception as e:
            logging.error("Error en plan():", exc_info=True)
            try:
                acciones_disponibles = self.policy.get_all_actions()
                if acciones_disponibles:
                    accion = acciones_disponibles[0]
                    logging.info("Se usa la primera acción disponible como acción por defecto.")
                else:
                    logging.error("La política no tiene acciones disponibles. No se puede definir acción por defecto.")
                    accion = None
            except Exception as ex:
                logging.error("Error al obtener acciones disponibles de la política:", exc_info=True)
                accion = None
        return accion


class AccionConfortTermico(pomdp_py.Action):
    def __init__(self, cambio_de_temperatura, cambio_de_flujo_de_aire=0):
        
        if not isinstance(cambio_de_temperatura, (int, float)):
            raise TypeError("[ERROR] 'cambio_de_temperatura' debe ser un número.")
        if not isinstance(cambio_de_flujo_de_aire, (int, float)):
            raise TypeError("[ERROR] 'cambio_de_flujo_de_aire' debe ser un número.")
        if not (-1 <= cambio_de_temperatura <= 1):
            raise ValueError(f"[ERROR] Valor fuera de rango: cambio_de_temperatura={cambio_de_temperatura}")
        if not (-1 <= cambio_de_flujo_de_aire <= 1):
            raise ValueError(f"[ERROR] Valor fuera de rango: cambio_de_flujo_de_aire={cambio_de_flujo_de_aire}")

        self.cambio_de_temperatura = cambio_de_temperatura
        self.cambio_de_flujo_de_aire = cambio_de_flujo_de_aire

    def __repr__(self):
        return (f"AccionConfortTermico(cambio_de_temperatura={self.cambio_de_temperatura}, "
                f"cambio_de_flujo_de_aire={self.cambio_de_flujo_de_aire})")

    def __eq__(self, other):
        return (isinstance(other, AccionConfortTermico) and 
                self.cambio_de_temperatura == other.cambio_de_temperatura and 
                self.cambio_de_flujo_de_aire == other.cambio_de_flujo_de_aire)

    def __hash__(self):
        return hash((self.cambio_de_temperatura, self.cambio_de_flujo_de_aire))

    def ejecutar_accion(self, estado):

        estado.ambiente["tdb"] += self.cambio_de_temperatura
        estado.control["vr"] += self.cambio_de_flujo_de_aire

        estado.ambiente["tdb"] = max(2, min(35, estado.ambiente["tdb"]))
        estado.control["vr"] = max(0, min(1, estado.control["vr"]))

        estado.pmv = estado.calcular_pmv()
        print(f"[DEBUG] Acción aplicada. Nuevo estado: temperatura={estado.ambiente['tdb']}, flujo={estado.control['vr']}")

    def es_accion_valida(self):
        
        return -1 <= self.cambio_de_temperatura <= 1 and -1 <= self.cambio_de_flujo_de_aire <= 1


class ObservacionConfort(pomdp_py.Observation):
    def __init__(self, tdb, tt, vr, Hr):

        self.tdb = max(5, min(40, tdb))
        self.tt = max(16, min(33, tt))
        self.vr = max(0, min(1, vr))
        self.Hr = max(10, min(80, Hr))

    def __repr__(self):
        return (f"ObservacionConfort(tdb={self.tdb:.2f}, tt={self.tt:.2f}, "
                f"vr={self.vr:.2f}, Hr={self.Hr:.2f})")

    def __eq__(self, other):
        return isinstance(other, ObservacionConfort) and (
            abs(self.tdb - other.tdb) < 1e-6 and
            abs(self.tt - other.tt) < 1e-6 and
            abs(self.vr - other.vr) < 1e-6 and
            abs(self.Hr - other.Hr) < 1e-6
        )

    def __hash__(self):
        return hash((
            round(self.tdb, 6),
            round(self.tt, 6),
            round(self.vr, 6),
            round(self.Hr, 6)
        ))


class Particula:
    def __init__(self, estado, weight=1.0):

        if isinstance(estado, Particula):
            self.estado = estado.estado
            self.weight = estado.weight
        else:
            self.estado = estado
            self.weight = weight

    def copy(self):
        return Particula(copy.deepcopy(self.estado), self.weight)

    def __repr__(self):
        return f"Particula(estado={self.estado}, weight={self.weight:.3f})"


class ModeloDeObservacion(pomdp_py.ObservationModel):
    def __init__(self, ruido_observacion=0.15):

        if not isinstance(ruido_observacion, (float, int)) or ruido_observacion < 0:
            raise ValueError("[ERROR] 'ruido_observacion' debe ser un número positivo.")

        self.ruido_observacion = max(0.2, min(1.0, ruido_observacion * 1.5))
        print(f"[DEBUG] ModeloDeObservacion inicializado con ruido_observacion={self.ruido_observacion}")

    def sample(self, estado, accion):

        try:
            if not isinstance(estado, EstadoConfortTermico):
                raise TypeError("[ERROR] 'estado' debe ser una instancia de EstadoConfortTermico.")
            
            Tt_valor = max(16, min(33, estado.control.get("Tt", 25) + np.random.normal(0, self.ruido_observacion)))
            tdb_valor = max(5, min(40, estado.ambiente.get("tdb", 25) + np.random.normal(0, self.ruido_observacion)))
            Var_valor = max(0, min(1, estado.control.get("vr", 0.5) + np.random.normal(0, self.ruido_observacion)))
            Hr_valor = max(10, min(80, estado.ambiente.get("Hr", 50) + np.random.normal(0, self.ruido_observacion)))
            
            observacion = ObservacionConfort(tdb=tdb_valor, tt=Tt_valor, vr=Var_valor, Hr=Hr_valor)
            print(f"[DEBUG] Observación generada correctamente: {observacion}")
            return observacion

        except Exception as e:
            print("[ERROR] Falló la generación de la observación:")
            raise e

    def probability(self, obs, estado, accion):

        try:
            if not isinstance(obs, ObservacionConfort):
                raise TypeError("[ERROR] 'obs' debe ser una instancia de ObservacionConfort.")
            if not isinstance(estado, EstadoConfortTermico):
                raise TypeError("[ERROR] 'estado' debe ser una instancia de EstadoConfortTermico.")
            
            sigma = max(1.0, min(15, self.ruido_observacion * 3))
            
            state_values = {
                "tdb": estado.ambiente.get("tdb", 25),
                "tt": estado.control.get("Tt", 25),
                "vr": estado.control.get("vr", 0.5),
                "Hr": estado.ambiente.get("Hr", 50)
            }
            
            probabilidad = np.mean([
                (1 / (sigma * np.sqrt(2 * np.pi))) *
                np.exp(-0.5 * ((getattr(obs, attr) - state_values[attr]) / sigma) ** 2)
                for attr in ["tdb", "tt", "vr", "Hr"]
            ])
            
            print(f"[DEBUG] Probabilidad calculada correctamente: {probabilidad}")
            return probabilidad

        except Exception as e:
            print("[ERROR] Falló el cálculo de probabilidad:")
            raise e

    def get_all_observations(self):

        try:
            observaciones = [
                ObservacionConfort(
                    tdb=np.random.normal(25, 5),
                    tt=np.random.normal(24, 4),
                    vr=np.random.normal(0.5, 0.2),
                    Hr=np.random.normal(50, 15)
                )
                for _ in range(100)
            ]
            print(f"[DEBUG] Se generaron {len(observaciones)} observaciones posibles.")
            return observaciones

        except Exception as e:
            print("[ERROR] Falló la generación de observaciones:")
            raise e


class ModeloBayesianoConfort:
    def __init__(self):
        """
        Inicializa el modelo bayesiano definiendo la estructura de la red.
        """
        self.model = BayesianModel([
            ('Tt', 'tdb'),
            ('tdb', 'Pa'),
            ('HR', 'Pa'),
            ('tdb', 'Tr'),
            ('Tcl', 'Hc'),
            ('HR', 'Tcl'),
            ('Pa', 'Tcl'),
            ('Tr', 'Tcl'),
            ('Icl', 'Tcl'),
            ('vr', 'Hc'),
            ('tdb', 'Hc'),
            ('Pa', 'PMV'),
            ('Tr', 'PMV'),
            ('Tcl', 'PMV'),
            ('Tt', 'PMV'),
            ('vr', 'PMV'),
            ('M', 'PMV'),
            ('W', 'PMV'),
            ('Icl', 'PMV')
        ])
    
    def construir_red(self):

        cpd_Tt = TabularCPD(variable='Tt', variable_card=2, values=[[0.3], [0.7]])
        cpd_HR = TabularCPD(variable='HR', variable_card=2, values=[[0.6], [0.4]])
        cpd_Var = TabularCPD(variable='vr', variable_card=2, values=[[0.5], [0.5]])
        cpd_M = TabularCPD(variable='M', variable_card=2, values=[[0.6], [0.4]])
        cpd_W = TabularCPD(variable='W', variable_card=2, values=[[0.5], [0.5]])
        cpd_Icl = TabularCPD(variable='Icl', variable_card=2, values=[[0.7], [0.3]])
        
        cpd_Ta = TabularCPD(
            variable='tdb',
            variable_card=2,
            values=[[0.8, 0.4],
                    [0.2, 0.6]],
            evidence=['Tt'],
            evidence_card=[2]
        )
        
        cpd_Pa = TabularCPD(
            variable='Pa',
            variable_card=2,
            values=[[0.7, 0.5, 0.6, 0.3],
                    [0.3, 0.5, 0.4, 0.7]],
            evidence=['tdb', 'HR'],
            evidence_card=[2, 2]
        )
        
        cpd_Tr = TabularCPD(
            variable='Tr',
            variable_card=2,
            values=[[0.8, 0.4],
                    [0.2, 0.6]],
            evidence=['tdb'],
            evidence_card=[2]
        )
        
        cpd_Tcl = TabularCPD(
            variable='Tcl',
            variable_card=2,
            values=[
                [0.7, 0.7, 0.5, 0.5, 0.4, 0.4, 0.2, 0.2, 0.3, 0.3, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02],
                [0.3, 0.3, 0.5, 0.5, 0.6, 0.6, 0.8, 0.8, 0.7, 0.7, 0.9, 0.9, 0.95, 0.95, 0.98, 0.98]
            ],
            evidence=['HR', 'Pa', 'Tr', 'Icl'],
            evidence_card=[2, 2, 2, 2]
        )
        
        cpd_Hc = TabularCPD(
            variable='Hc',
            variable_card=2,
            values=[
                [0.9, 0.9, 0.6, 0.6, 0.7, 0.7, 0.3, 0.3],
                [0.1, 0.1, 0.4, 0.4, 0.3, 0.3, 0.7, 0.7]
            ],
            evidence=['Tcl', 'vr', 'tdb'],
            evidence_card=[2, 2, 2]
        )
        
        cpd_PMV = TabularCPD(
            variable='PMV',
            variable_card=2,
            values=np.full((2, 256), 0.5).tolist(),
            evidence=['Pa','Tr','Tcl','Tt','vr','M','W','Icl'],
            evidence_card=[2, 2, 2, 2, 2, 2, 2, 2]
        )
        
        self.model.add_cpds(
            cpd_Tt, cpd_HR, cpd_Var, cpd_M, cpd_W, cpd_Icl,
            cpd_Ta, cpd_Pa, cpd_Tr, cpd_Tcl, cpd_Hc, cpd_PMV
        )
        
        if not self.model.check_model():
            raise Exception("El modelo Bayesiano no es válido.")
        
        self.infer = VariableElimination(self.model)
    
    def inferir_pmv(self, evidencia):

        query_result = self.infer.query(variables=['PMV'], evidence=evidencia)
        return query_result['PMV']



def calcular_pmv(estado):

    tdb = estado.ambiente.get("tdb", 25)
    tr = estado.ambiente.get("Tr", tdb)
    rh = estado.ambiente.get("HR", estado.ambiente.get("Hr", 50))
    met = estado.persona.get("M", 1.2)
    clo = estado.persona.get("Icl", 0.5)
    wme = estado.persona.get("W", 0.0)
    vel = estado.ambiente.get("vr", 0.1)

    resultado = pmv_ppd_iso(tdb=tdb, tr=tr, vel=vel, rh=rh, met=met, clo=clo, wme=wme)
    return resultado['pmv']


class ModeloDeTransicion(pomdp_py.TransitionModel):
    def __init__(self, estados=None):
        super().__init__()
        logging.info("DEBUG: Se ha creado una instancia de ModeloDeTransicion")
        self.estados = estados if estados else []

        try:
            from modelo_bayesiano import ModeloBayesianoConfort
            self.modelo_bayesiano = ModeloBayesianoConfort()
            self.modelo_bayesiano.construir_red()
            logging.info("DEBUG: Modelo bayesiano inicializado correctamente.")
        except Exception as e:
            logging.error("No se pudo inicializar el modelo bayesiano: " + str(e))
            self.modelo_bayesiano = None 

    def probability(self, estado_siguiente, estado_actual, accion):
        
        try:
            logging.info("DEBUG: Entré en probability")
            
            def compute_soft_evidence(value, threshold, sigma):
                logging.info(f"DEBUG: compute_soft_evidence - value: {value}, threshold: {threshold}, sigma: {sigma}")
                p0 = np.exp(-0.5 * ((value - threshold) / sigma) ** 2)
                p1 = 1 - p0
                norm = p0 + p1 if (p0 + p1) != 0 else 1.0
                result = [p0 / norm, p1 / norm]
                logging.info(f"DEBUG: compute_soft_evidence resultado: {result}")
                return result

            thresholds = {"tdb": 25, "Tt": 25, "vr": 0.5, "HR": 50}
            sigma_vals = {"tdb": 2.0, "Tt": 2.0, "vr": 0.1, "HR": 10.0}
            keys = ["tdb", "Tt", "vr", "HR"]

            soft_evidence = {
                "tdb": compute_soft_evidence(estado_actual.ambiente.get("tdb", 25), thresholds["tdb"], sigma_vals["tdb"]),
                "Tt": compute_soft_evidence(estado_actual.control.get("Tt", 25), thresholds["Tt"], sigma_vals["Tt"]),
                "vr": compute_soft_evidence(estado_actual.control.get("vr", 0.5), thresholds["vr"], sigma_vals["vr"]),
                "HR": compute_soft_evidence(estado_actual.ambiente.get("HR", 50), thresholds["HR"], sigma_vals["HR"])
            }
            logging.info(f"DEBUG: Soft evidence: {soft_evidence}")

            total_prob = 0.0

            for combination in product([0, 1], repeat=4):
                weight = 1.0
                evidence = {}
                for i, key in enumerate(keys):
                    weight *= soft_evidence[key][combination[i]]
                    evidence[key] = combination[i]
                logging.info(f"DEBUG: Evidencia: {evidence}, Peso parcial: {weight:.8f}")

                pmv_distribution = (self.modelo_bayesiano.inferir_pmv(evidence)
                                      if self.modelo_bayesiano is not None else None)

                estado_siguiente.pmv = estado_siguiente.calcular_pmv()

                observed_state = 0 if estado_siguiente.pmv < 0 else 1

                try:
                    contrib = weight * pmv_distribution.get_value(PMV=observed_state)
                except Exception as ex:
                    logging.error("DEBUG: Error al usar get_value(): " + str(ex))
                    try:
                        contrib = weight * pmv_distribution.values[observed_state]
                    except Exception as ex2:
                        logging.error("DEBUG: Error al acceder a pmv_distribution.values: " + str(ex2))
                        contrib = 0.0

                logging.info(f"DEBUG: Contribución de esta combinación: {contrib:.8f}")
                total_prob += contrib

            logging.info(f"DEBUG: Probabilidad total: {total_prob:.8f}")
            return total_prob

        except Exception as e:
            logging.error("[ERROR] Falló la inferencia bayesiana en transición con soft evidence: " + str(e))
            return 0.0


class ModeloDeRecompensa(pomdp_py.RewardModel):
    def __init__(self, pmv_deseado):

        if not isinstance(pmv_deseado, (int, float)):
            raise ValueError("[ERROR] 'pmv_deseado' debe ser un número válido.")
        self.pmv_deseado = pmv_deseado

    def _funcion_recompensa(self, estado, accion):

        if estado is None or not isinstance(estado, EstadoConfortTermico):
            print("[WARNING] Estado recibido no es válido. Generando uno nuevo...")
            estado = EstadoConfortTermico.generar_estado_aleatorio()

        if accion is None or not isinstance(accion, AccionConfortTermico):
            print("[WARNING] Acción inválida, asignando acción por defecto y recompensa 0.0.")
            return 0.0  

        pmv_actual = estado.pmv
        recompensa = 10 - (pmv_actual - self.pmv_deseado) ** 2

        if accion.cambio_de_temperatura > 0 and estado.ambiente.get("tdb", 0) >= estado.control.get("Tt", 0):
            recompensa -= 2
        elif accion.cambio_de_temperatura < 0 and estado.ambiente.get("tdb", 0) <= estado.control.get("Tt", 0):
            recompensa -= 2
        if accion.cambio_de_flujo_de_aire > 0 and estado.transferencia.get("hc", 0) >= 1.0:
            recompensa -= 1
        elif accion.cambio_de_flujo_de_aire < 0 and estado.transferencia.get("hc", 0) <= 0.5:
            recompensa -= 1  
        if abs(pmv_actual) > 0.7:
            penalizacion = abs(pmv_actual) * 1.5
            recompensa -= penalizacion

        recompensa = max(0.0, recompensa)
        print(f"[DEBUG] Recompensa calculada: {recompensa}")
        return recompensa

    def sample(self, estado, accion, estado_siguiente):

        return self._funcion_recompensa(estado, accion)


class ModeloDePolitica(pomdp_py.RolloutPolicy):
    def __init__(self, pmv_deseado=0.0):

        super().__init__()
        self.pmv_deseado = pmv_deseado
        self.acciones = [
            AccionConfortTermico(cambio_de_temperatura=1,  cambio_de_flujo_de_aire=0),  
            AccionConfortTermico(cambio_de_temperatura=-1, cambio_de_flujo_de_aire=0), 
            AccionConfortTermico(cambio_de_temperatura=0,  cambio_de_flujo_de_aire=1),   
            AccionConfortTermico(cambio_de_temperatura=0,  cambio_de_flujo_de_aire=-1),  
            AccionConfortTermico(cambio_de_temperatura=0,  cambio_de_flujo_de_aire=0)
        ]

    def sample(self, estado):

        if not self.acciones:
            print("[WARNING] sample: No hay acciones disponibles; asignando acción por defecto.")
            return AccionConfortTermico(cambio_de_temperatura=0, cambio_de_flujo_de_aire=0)
        
        accion_seleccionada = random.choice(self.acciones)
        if accion_seleccionada is None:
            print("[WARNING] sample: La acción seleccionada es None; asignando acción por defecto.")
            return AccionConfortTermico(cambio_de_temperatura=0, cambio_de_flujo_de_aire=0)
        
        print(f"[DEBUG] Acción seleccionada en sample: {accion_seleccionada}")
        return accion_seleccionada

    def rollout(self, estado, historial=None):

        accion_durante_rollout = self.sample(estado)
        print(f"[DEBUG] Acción utilizada durante rollout: {accion_durante_rollout}")
        return accion_durante_rollout

    def get_all_actions(self, state=None, history=None):

        print(f"[DEBUG] Acciones disponibles en la política: {self.acciones}")
        return self.acciones


class EntornoConfort(pomdp_py.Environment):
    def __init__(self, estados, estado_inicial):

        if not isinstance(estados, (list, set)) or not all(isinstance(e, EstadoConfortTermico) for e in estados):
            raise ValueError("[ERROR] 'estados' debe ser una lista o conjunto de instancias de EstadoConfortTermico.")
        if not isinstance(estado_inicial, EstadoConfortTermico):
            raise ValueError("[ERROR] 'estado_inicial' debe ser una instancia válida de EstadoConfortTermico.")

        self.estados = copy.deepcopy(estados)
        self.estado_inicial = estado_inicial

        super().__init__(estado_inicial)
        print(f"[DEBUG] EntornoConfort inicializado con {len(self.estados)} estados posibles.")

    def estados_posibles(self):

        return self.estados

    def __repr__(self):

        estados_ejemplo = self.estados[:3] if len(self.estados) > 3 else self.estados
        return f"EntornoConfort(estado_inicial={self.estado_inicial}, estados={len(self.estados)} estados, ejemplo={estados_ejemplo})"    


class ProblemaConfortTermico(pomdp_py.POMDP):
    def __init__(self, agente, entorno):

        if not isinstance(agente, AgentePersonalizado):
            raise ValueError("[ERROR] 'agente' debe ser una instancia de AgentePersonalizado.")
        if not isinstance(entorno, EntornoConfort):
            raise ValueError("[ERROR] 'entorno' debe ser una instancia de EntornoConfort.")

        super().__init__(agente, entorno)
        self.agente = agente
        self.entorno = entorno

        print("[DEBUG] ProblemaConfortTermico inicializado correctamente.")

    @staticmethod
    def crear(ruido_observacion, pmv_deseado):

        try:
            print("[DEBUG] Configurando el problema de confort térmico...")

            creencia_inicial = crear_creencia_inicial_en_particulas(num_particulas=1000)
            if not isinstance(creencia_inicial, pomdp_py.Particles):
                raise ValueError("[ERROR] La creencia inicial no es válida.")

            policy_model = ModeloDePolitica(pmv_deseado=pmv_deseado)
            transition_model = ModeloDeTransicion()
            observation_model = ModeloDeObservacion(ruido_observacion=ruido_observacion)
            reward_model = ModeloDeRecompensa(pmv_deseado=pmv_deseado)

            agente = AgentePersonalizado(
                nombre="AgenteConfortTermico",
                pmv=pmv_deseado,
                belief=creencia_inicial,
                policy=policy_model,
                transition_model=transition_model,
                observation_model=observation_model,
                reward_model=reward_model
            )
            print("[DEBUG] Agente configurado con éxito.")

            estados_posibles = [EstadoConfortTermico.generar_estado_aleatorio() for _ in range(50)]
            estado_inicial = estados_posibles[np.random.randint(0, len(estados_posibles))]

            entorno = EntornoConfort(estados=estados_posibles, estado_inicial=estado_inicial)
            print("[DEBUG] Entorno configurado con éxito.")

            problema = ProblemaConfortTermico(agente, entorno)
            print("[DEBUG] ProblemaConfortTermico creado con éxito.")
            return problema

        except Exception as e:
            print("[ERROR] Ocurrió un error durante la creación del problema:")
            traceback.print_exc()
            raise e



def crear_creencia_inicial_en_particulas(num_particulas=5000):
    try:
        logging.info("[DEBUG] Comenzando a crear creencia inicial con partículas...")
        if num_particulas <= 0:
            raise ValueError("[ERROR] 'num_particulas' debe ser un número positivo.")

        estados_iniciales = []
        for _ in range(num_particulas):
            ambiente = {
                "Hr": round(random.uniform(10, 90), 0),
                "Pa": round(random.uniform(500, 5600), 0),
                "tdb": round(np.clip(random.uniform(5, 40), 0, 40), 0),
                "Tr": round(np.clip(np.random.normal(25, 5), 10, 40), 0)
            }
            control = {
                "Tt": round(random.uniform(16, 33), 0),
                "vr": round(np.clip(random.uniform(0, 2), 0, 2), 1)
            }
            persona = {
                "M": np.clip(np.random.normal(1.5, 0.4), 0.5, 4.5),
                "W": np.clip(np.random.normal(0.2, 0.06), 0, 0.4),
                "Icl": np.clip(np.random.normal(1.5, 0.7), 0.2, 3)
            }
            transferencia = {
                "hc": np.clip(np.random.normal(3, 0.5), 1, 6),
                "Tcl": np.clip(np.random.normal(25, 3), 5, 35)
            }
            pmv = 0.0

            estado = EstadoConfortTermico(ambiente, control, persona, transferencia, pmv)
            estados_iniciales.append(estado)

        particulas = [
            Particula(estado=estado, weight=np.random.uniform(0.8, 1.5))
            for estado in estados_iniciales
        ]
        logging.info(f"[DEBUG] Partículas creadas con éxito: {len(particulas)} partículas.")

        belief = pomdp_py.Particles(particulas)

        total_weight = sum(p.weight for p in belief.particles)
        if total_weight == 0:
            logging.error("[ERROR] Todos los pesos son cero, reajustando...")
            for p in belief.particles:
                p.weight = np.random.uniform(1.2, 2)
            total_weight = sum(p.weight for p in belief.particles)
        else:
            for p in belief.particles:
                normalized_value = p.weight / total_weight
                p.weight = normalized_value * np.random.uniform(0.9, 1.1)

        n_eff = 1.0 / (sum(p.weight**2 for p in belief.particles) + 1e-6)
        logging.info(f"[DEBUG] n_eff calculado: {n_eff}")
        if n_eff < len(belief.particles) * 0.5:
            logging.info(f"[DEBUG] Resampling activado - Número efectivo de partículas: {n_eff}")
            particulas_resampleadas = random.choices(
                belief.particles, weights=[p.weight for p in belief.particles], k=len(belief.particles)
            )
            if n_eff < len(belief.particles) * 0.3:
                cantidad_nuevas = int(len(belief.particles) * 0.1)
                logging.info(f"[DEBUG] n_eff crítico. Añadiendo {cantidad_nuevas} nuevas partículas para diversificar.")
                nuevas_particulas = [
                    Particula(EstadoConfortTermico.generar_estado_aleatorio(), weight=np.random.uniform(0.8, 1.5))
                    for _ in range(cantidad_nuevas)
                ]
                particulas_resampleadas.extend(nuevas_particulas)
            for p in particulas_resampleadas:
                p.weight = np.random.uniform(0.8, 1.5)
            belief = pomdp_py.Particles(particulas_resampleadas)

        # --- Ajuste dinámico del roughening ---
        # 1. Medir la diversidad: calcular la varianza de 'tdb' en el belief
        tdb_values = [p.estado.ambiente.get("tdb", 25) for p in belief.particles]
        variance_tdb = np.var(tdb_values)
        # 2. Definir parámetros base y umbral
        umbral_minimo = 1.0               # Varianza mínima deseada (ajusta según tu dominio)
        base_sigma_ambiente = 40.0         # Valor base para sigma ambiente (según tus experimentos)
        epsilon = 1e-6
        # 3. Ajustar sigma de forma dinámica
        if variance_tdb < umbral_minimo:
            sigma_ambiente = base_sigma_ambiente * (umbral_minimo / (variance_tdb + epsilon))
        else:
            sigma_ambiente = base_sigma_ambiente
        # Para el control usamos un valor fijo o también se podría ajustar dinámicamente
        sigma_control = 22.0

        logging.info(f"[DEBUG] Variance tdb: {variance_tdb}, sigma_ambiente ajustado: {sigma_ambiente}")

        # Aplicar roughening dinámico (hacer perturbaciones en 'tdb' y 'Tr' usando el sigma dinámico)
        for p in belief.particles:
            if "tdb" in p.estado.ambiente:
                p.estado.ambiente["tdb"] += np.random.normal(0, sigma_ambiente)
            if "Tr" in p.estado.ambiente:
                p.estado.ambiente["Tr"] += np.random.normal(0, sigma_ambiente)
            # Aplicar perturbación al control (usando sigma_control fijo en este ejemplo)
            if hasattr(p.estado, "control"):
                for key, value in p.estado.control.items():
                    try:
                        p.estado.control[key] = value + np.random.normal(0, sigma_control)
                    except Exception as ex:
                        print(f"[WARNING] No se pudo perturbar {key} en control: {ex}")
            p.estado = validate_estado(p.estado)
        # --- Fin del ajuste dinámico del roughening ---

        logging.info("[DEBUG] Creencia inicial generada con éxito.")
        return belief

    except Exception as e:
        logging.error("[ERROR] Ocurrió un error durante la creación de la creencia inicial: " + str(e))
        raise e

def RS_resample(N, weights, particles):

    cumulative_sum = np.cumsum(weights)
    U = np.random.uniform(0, 1.0 / N)
    positions = U + np.arange(N) / N
    indexes = np.searchsorted(cumulative_sum, positions)
    indexes = [min(idx, len(particles) - 1) for idx in indexes]
    new_particles = [particles[int(i)].copy() for i in indexes]
    for p in new_particles:
        p.weight = 1.0 / N
    return new_particles

def calculate_n_eff(particles):

    weights = np.array([p.weight for p in particles])
    n_eff = 1.0 / np.sum(np.square(weights)) if np.sum(np.square(weights)) > 0 else 0
    return n_eff

def systematic_resample(particles):

    N = len(particles)
    positions = (np.arange(N) + random.random()) / N
    cumulative_sum = np.cumsum([p.weight for p in particles])
    cumulative_sum[-1] = 1.0 
    indexes = []
    i, j = 0, 0
    while i < N:
        if positions[i] < cumulative_sum[j]:
            indexes.append(j)
            i += 1
        else:
            j += 1
    new_particles = [particles[i].copy() for i in indexes]
    for p in new_particles:
        p.weight = 1.0 / N
    return new_particles


def roughening(particles, K=1.8):

    if not particles:
        return particles

    import random
    N = len(particles)
    state_attrs = list(particles[0].estado.__dict__.keys())
    subdicts = [attr for attr in state_attrs if isinstance(getattr(particles[0].estado, attr), dict)]
    
    sigma = {}
    for sub in subdicts:
        keys = list(getattr(particles[0].estado, sub).keys())
        sigma[sub] = {}
        for key in keys:
            values = []
            for p in particles:
                val = getattr(p.estado, sub).get(key, None)
                if val is not None:
                    values.append(val)
            if values:
                sigma[sub][key] = K * (max(values) - min(values)) * (N ** (-1.0))
    
    for p in particles:
        for sub in subdicts:
            sub_dict = getattr(p.estado, sub)
            for key in sub_dict.keys():
                if key in sigma[sub]:
                    noise = random.gauss(0, sigma[sub][key])
                    sub_dict[key] += noise
    return particles

def update_particles(particles, n_eff_threshold):

    n_eff = calculate_n_eff(particles)
    logging.debug(f"n_eff calculado: {n_eff}")
    
    if n_eff < n_eff_threshold:
        logging.info(f"n_eff {n_eff} es menor al umbral {n_eff_threshold}. Re-muestreando...")
        particles = systematic_resample(particles)
        particles = roughening(particles, K=0.5)
    return particles


def validate_estado(estado):
    if "tdb" in estado.ambiente:
        estado.ambiente["tdb"] = np.clip(estado.ambiente["tdb"], 15, 30)
    if "Tr" in estado.ambiente:
        estado.ambiente["Tr"] = np.clip(estado.ambiente["Tr"], 15, 35)
    if "Hr" in estado.ambiente:
        estado.ambiente["Hr"] = np.clip(estado.ambiente["Hr"], 20, 90)
    if "Pa" in estado.ambiente:
        estado.ambiente["Pa"] = np.clip(estado.ambiente["Pa"], 800, 1200)  
    if "vr" in estado.control:
        estado.control["vr"] = np.clip(estado.control["vr"],0, 2)    
    return estado


def observacion_a_vector(observacion):
    """
    Convierte una observación a un vector numérico.
    Si la observación es un diccionario (como la que genera el método simular_observacion),
    se espera que tenga las claves 'tdb', 'Tr', 'Hr' y 'Pa'.
    Si es una instancia de ObservacionConfort, se deben extraer sus atributos relevantes.
    Ajusta este método según la estructura real de ObservacionConfort.
    """
    if isinstance(observacion, dict):
        orden = ["tdb", "Tr", "Hr", "Pa"]
        return np.array([observacion.get(clave, 0) for clave in orden])
    elif hasattr(observacion, '__dict__'):
        return np.array([
            getattr(observacion, 'tdb', 0),
            getattr(observacion, 'Tr', 0),
            getattr(observacion, 'Hr', 0),
            getattr(observacion, 'Pa', 0)
        ])
    else:
        raise TypeError("Tipo de observacion no soportado")


def update_belief_with_resample(agente, accion, observacion_real):
    global iteraciones_resampleo
    if "iteraciones_resampleo" not in globals():
        iteraciones_resampleo = 0

    if not hasattr(agente, "policy") or not hasattr(agente, "cur_belief"):
        print("[ERROR] El agente no tiene una política o creencia válida.")
        return

    try:
        acciones_disponibles = agente.policy.get_all_actions()
        if not acciones_disponibles:
            raise ValueError("[ERROR] La política del agente no tiene acciones disponibles.")
    except Exception as e:
        print(f"[WARNING] Fallo al obtener acciones de la política: {e}")
        return

    agente.RS_resample = RS_resample
    particulas_seguras = list(agente.cur_belief.particles)
    particulas_corregidas = [
        Particula(p) if isinstance(p, EstadoConfortTermico) else p
        for p in particulas_seguras
    ]
    particulas_corregidas = [p for p in particulas_corregidas if isinstance(p, Particula)]

    for p in particulas_corregidas:
        if not isinstance(p.estado, EstadoConfortTermico):
            p.estado = EstadoConfortTermico.generar_estado_aleatorio()
        if np.isnan(p.estado.pmv):
            print("[WARNING] pmv es NaN en una partícula, reemplazándolo por 0.0")
            p.estado.pmv = 0.0

    agente.set_belief(pomdp_py.Particles(particulas_corregidas))

    # --- Validación del Modelo de Observación ---
    # Se asume que el método simular_observacion() está definido en la clase EstadoConfortTermico
    diffs = []
    for i, p in enumerate(particulas_corregidas):
        obs_sim = p.estado.simular_observacion()
        # Convertir las observaciones a vectores con el mismo orden (definido en observacion_a_vector)
        vec_sim = observacion_a_vector(obs_sim)
        vec_real = observacion_a_vector(observacion_real)
        # Para la primera partícula, se registra el contenido para inspección.
        if i == 0:
            logging.info(f"[DEBUG] Observación simulada (diccionario) de la primera partícula: {obs_sim}")
            logging.info(f"[DEBUG] Observación simulada convertida a vector: {vec_sim}")
            logging.info(f"[DEBUG] Observación real (diccionario): {observacion_real}")
            logging.info(f"[DEBUG] Observación real convertida a vector: {vec_real}")
        diff = np.linalg.norm(vec_sim - vec_real)
        diffs.append(diff)

    # Calcular var_diff y ajustar el ruido de observación
    var_diff = np.var(diffs)
    umbral_obs = 0.5        # Umbral deseado para la varianza de las diferencias (ajusta según tu dominio)
    base_ruido_obs = 0.15     # Valor base para el ruido observacional

    if var_diff < 1e-6:  # Si la varianza es muy pequeña (prácticamente cero)
        ruido_observacion_ajustado = base_ruido_obs
    elif var_diff < umbral_obs:
        ruido_observacion_ajustado = base_ruido_obs * (umbral_obs / (var_diff + 1e-6))
    else:
        ruido_observacion_ajustado = base_ruido_obs

    logging.info(f"[DEBUG] Varianza de diferencias observacionales: {var_diff}, ruido observacion ajustado: {ruido_observacion_ajustado}")

    # Crear el modelo de observación usando el ruido ajustado
    modelo_obs = ModeloDeObservacion(ruido_observacion=ruido_observacion_ajustado)
    for p in particulas_corregidas:
        likelihood = modelo_obs.probability(observacion_real, p.estado, accion)
        p.weight = likelihood

    # Continúa con los siguientes pasos...
    # Filtrar partículas con valores NaN antes del re-muestreo
    particulas_corregidas = [p for p in particulas_corregidas if not np.isnan(p.estado.pmv)]
    epsilon = 1e-6
    peso_total = sum(p.weight for p in particulas_corregidas) + epsilon
    if peso_total < 1e-3:
        print("[ERROR] Los pesos son demasiado bajos antes de re-muestreo. Ajustando...")
        for p in particulas_corregidas:
            p.weight += np.random.uniform(0.01, 0.1)
        peso_total = sum(p.weight for p in particulas_corregidas) + epsilon
    for p in particulas_corregidas:
        p.weight = p.weight / peso_total

    n_eff = 1.0 / (sum(p.weight**2 for p in particulas_corregidas) + epsilon)
    print(f"[DEBUG] n_eff calculado antes del re-muestreo: {n_eff}")

    particulas_corregidas = update_particles(particulas_corregidas, n_eff_threshold=0.5 * len(particulas_corregidas))
    n_eff = calculate_n_eff(particulas_corregidas)
    print(f"[DEBUG] n_eff tras update_particles: {n_eff}")

    if n_eff < max(150, len(particulas_corregidas) * 0.7):
        print("[DEBUG] n_eff crítico. Generando nuevas partículas aleatorias...")
        num_nuevas = 1000
        new_particles = [
            Particula(EstadoConfortTermico.generar_estado_aleatorio(), weight=np.random.uniform(10, 20))
            for _ in range(num_nuevas)
        ]
        particulas_corregidas.extend(new_particles)

    W_total = sum(p.weight for p in particulas_corregidas) + epsilon
    for p in particulas_corregidas:
        p.weight = p.weight / W_total

    for p in particulas_corregidas:
        p.weight *= np.random.uniform(0.8, 1.2) if n_eff < 700 else np.random.uniform(0.95, 1.05)

    desired_N = 1000
    if len(particulas_corregidas) < desired_N:
        faltantes = desired_N - len(particulas_corregidas)
        print(f"[DEBUG] Faltan {faltantes} partículas, generando partículas nuevas.")
        nuevas_particulas = [
            Particula(EstadoConfortTermico.generar_estado_aleatorio(), weight=1.0)
            for _ in range(faltantes)
        ]
        particulas_corregidas.extend(nuevas_particulas)
        tot = sum(p.weight for p in particulas_corregidas) + epsilon
        for p in particulas_corregidas:
            p.weight = p.weight / tot

    final_particles = RS_resample(desired_N, [p.weight for p in particulas_corregidas], particulas_corregidas)
    for p in final_particles:
        p.weight = 1.0 / desired_N

    # --- Paso 4: Ajuste dinámico del roughening ---
    # Ajuste dinámico para el ambiente: medir la diversidad en 'tdb'
    tdb_values = [p.estado.ambiente.get("tdb", 25) for p in final_particles]
    variance_tdb = np.var(tdb_values)
    umbral_minimo = 1.0               # Varianza mínima deseada; ajusta según tu dominio
    base_sigma_ambiente = 40.0         # Valor base para sigma ambiente; según experimentación
    if variance_tdb < umbral_minimo:
        sigma_ambiente = base_sigma_ambiente * (umbral_minimo / (variance_tdb + epsilon))
    else:
        sigma_ambiente = base_sigma_ambiente

    # Ajuste dinámico para el control: medir la diversidad en el parámetro 'vr'
    vr_values = [p.estado.control.get("vr", 0.1) for p in final_particles if "vr" in p.estado.control]
    variance_vr = np.var(vr_values)
    umbral_minimo_control = 0.1       # Varianza mínima deseada para 'vr'; ajusta según tu dominio
    base_sigma_control = 22.0         # Valor base para sigma control
    if variance_vr < umbral_minimo_control:
        sigma_control = base_sigma_control * (umbral_minimo_control / (variance_vr + epsilon))
    else:
        sigma_control = base_sigma_control

    logging.info(f"[DEBUG] Variance tdb: {variance_tdb}, sigma_ambiente ajustado: {sigma_ambiente}")
    logging.info(f"[DEBUG] Variance vr: {variance_vr}, sigma_control ajustado: {sigma_control}")

    # Aplicar una perturbación leve en 'tdb' y 'Tr' para promover diversidad
    for p in final_particles:
        if "tdb" in p.estado.ambiente:
            p.estado.ambiente["tdb"] += np.random.normal(0, 2)
        if "Tr" in p.estado.ambiente:
            p.estado.ambiente["Tr"] += np.random.normal(0, 2)

    # Aplicar perturbaciones generales usando los sigma ajustados
    for p in final_particles:
        if hasattr(p.estado, "ambiente"):
            for key, value in p.estado.ambiente.items():
                try:
                    p.estado.ambiente[key] = value + np.random.normal(0, sigma_ambiente)
                except Exception as ex:
                    print(f"[WARNING] No se pudo perturbar {key} en ambiente: {ex}")
        if hasattr(p.estado, "control"):
            for key, value in p.estado.control.items():
                try:
                    p.estado.control[key] = value + np.random.normal(0, sigma_control)
                except Exception as ex:
                    print(f"[WARNING] No se pudo perturbar {key} en control: {ex}")

    for p in final_particles:
        p.estado = validate_estado(p.estado)

    agente.set_belief(pomdp_py.Particles(final_particles))
    print(f"[DEBUG] Creencia final re-sampleada a {desired_N} partículas con roughening.")

    # Si no quedan partículas válidas, se genera una de respaldo.
    if len(agente.cur_belief.particles) == 0:
        print("[WARNING] No hay partículas válidas. Generando un estado aleatorio.")
        agente.set_belief(pomdp_py.Particles([EstadoConfortTermico.generar_estado_aleatorio()]))

    for idx, p in enumerate(agente.cur_belief.particles[:5]):
        print(f"[DEBUG] Partícula {idx}: Tipo de estado: {type(p.estado)}, estado: {p.estado}")


def prueba_basica():
    print("[INFO] Ejecutando prueba básica del sistema POMDP.")

    try:

        ambiente = {"Hr": 38.0, "Pa": 93.9, "tdb": 24, "Tr": 25.3}
        control = {"Tt": 20, "vr": 0.75}
        persona = {"M": 1.05, "W": 0.18, "Icl": 1.86}
        transferencia = {"hc": 0.92, "Tcl": 22.4}
        pmv = 1.0

        print("[DEBUG] Comenzando la inicialización de EstadoConfortTermico...")
        try:
            estado_inicial = EstadoConfortTermico(ambiente, control, persona, transferencia, pmv)
            print("[DEBUG] Estado inicial creado correctamente:", estado_inicial)
        except Exception as e:
            print(f"[ERROR] Fallo en la inicialización de EstadoConfortTermico: {e}")
            return

        modelo_transicion = ModeloDeTransicion()
        print("[DEBUG] Modelo de Transición creado:", modelo_transicion)

        try:
            print("[TEST] Probando crear_creencia_inicial_en_particulas...")
            creencia = crear_creencia_inicial_en_particulas(num_particulas=1000)  
            print("[TEST] Creencia inicial creada con éxito:", creencia)

            pesos = [p.weight for p in creencia.particles]
            print(f"[DEBUG] Rango de pesos en creencia inicial: min={min(pesos)}, max={max(pesos)}, promedio={sum(pesos) / len(pesos)}")

            if min(pesos) < 0.5:
                print("[WARNING] Algunos pesos son demasiado bajos. Ajustando...")
                for p in creencia.particles:
                    p.weight = max(0.5, p.weight)  

        except Exception as e:
            print(f"[TEST ERROR] La prueba de creencia inicial falló: {type(e).__name__}: {e}")
            return  

        if not isinstance(ModeloDePolitica(), pomdp_py.Policy):
            print("[ERROR] 'ModeloDePolitica' no es una política válida.")
            return

        try:
            pouct = pomdp_py.POUCT(
                max_depth=5,
                discount_factor=0.95,
                num_sims=100, 
                exploration_const=1.0,
                rollout_policy=ModeloDePolitica()
            )
            print("[DEBUG] Planificador POUCT configurado:", pouct)
        except Exception as e:
            print(f"[ERROR] Fallo en la configuración de POUCT: {e}")
            return

        try:
            estados_posibles = [estado_inicial]
            entorno = EntornoConfort(estados=estados_posibles, estado_inicial=estado_inicial)
            print("[DEBUG] Entorno creado correctamente:", entorno)
        except Exception as e:
            print(f"[ERROR] Fallo en la creación del entorno: {e}")
            return

        if not isinstance(entorno, pomdp_py.Environment):
            print("[ERROR] 'entorno' no es una instancia válida de pomdp_py.Environment.")
            return

        try:
            accion = pouct.plan(entorno)
            print(f"[DEBUG] Acción seleccionada por el planificador: {accion}")
        except Exception as e:
            print(f"[ERROR] Fallo en la planificación con POUCT: {e}")

    except Exception as e:
        print(f"[ERROR] Ocurrió un error durante la prueba básica: {e}")

    print("[INFO] Prueba básica completada.")

def convert_belief_to_state_list(belief):

    state_list = []
    for p in belief.particles:
        if hasattr(p, 'estado'):
            state_list.append(p.estado)
        else:
            state_list.append(p)
    return state_list


def safe_plan_call(planificador, agente):
    import logging
    logging.info("[DEBUG] Verificando agente y planificador antes de ejecutar plan()...")

    if not hasattr(agente, "cur_belief") or not hasattr(agente, "policy"):
        logging.error("[ERROR] El objeto agente no contiene las propiedades necesarias (cur_belief, policy).")
        return AccionConfortTermico(cambio_de_temperatura=0, cambio_de_flujo_de_aire=0)
    
    logging.info(f"DEBUG: Tipo de creencia del agente: {type(agente.cur_belief)}")

    if not agente.cur_belief.particles:
        logging.warning("[WARNING] `belief.particles` está vacío, generando partículas de respaldo...")
        agente.set_belief(Particles([
            Particula(EstadoConfortTermico.generar_estado_aleatorio(), weight=1.0)
        ]))
    
    particulas_seguras = [p for p in agente.cur_belief.particles if isinstance(p, Particula)]
    if not particulas_seguras:
        logging.warning("[WARNING] No hay partículas válidas después del filtrado. Se generará una de respaldo.")
        particulas_seguras.append(Particula(EstadoConfortTermico.generar_estado_aleatorio(), weight=1.0))
    
    # Ajuste de pesos: se limita el rango y luego se normalizan
    for p in particulas_seguras:
        p.weight = max(min(p.weight, 1.5), 0.5)
    peso_total = sum(p.weight for p in particulas_seguras) or 1.0
    for p in particulas_seguras:
        p.weight /= peso_total

    # Unificamos el proceso de actualización del belief:
    state_list = convert_belief_to_state_list(Particles(particulas_seguras))
    agente.set_belief(Particles(state_list))
    agente.cur_belief_states = state_list

    for idx, s in enumerate(agente.cur_belief.particles[:5]):
        logging.info(f"[DEBUG] Elemento {idx}: Tipo: {type(s)}, contenido: {s}")
    
    try:
        resultado = planificador.plan(agente)
        if resultado is None:
            raise ValueError("[ERROR] El planificador devolvió None.")
    except Exception as e:
        logging.warning("Se produjo una excepción en plan(): " + str(e))
        return AccionConfortTermico(cambio_de_temperatura=0, cambio_de_flujo_de_aire=0)
    
    logging.info(f"DEBUG: Acción planificada con éxito: {resultado}")
    return resultado


def probar_planificador_con_arbol(problema_confort, planificador, pasos=5):

    print(f"[DEBUG] Probando planificador {type(planificador).__name__} con árbol...")
    
    tiene_arbol = hasattr(planificador, 'tree') and planificador.tree is not None
    print(f"[DEBUG] ¿Planificador tiene árbol?: {tiene_arbol}")
    
    if tiene_arbol:
        try:
            print("[DEBUG] ** Inspeccionando el árbol con TreeDebugger **")
            debugger = TreeDebugger(planificador.tree)
            debugger.display(width=80)  
        except Exception as e:
            print(f"[ERROR] Fallo al inspeccionar el árbol con TreeDebugger: {e}")
    
    accion = safe_plan_call(planificador, problema_confort.agente)
    
    for i in tqdm(range(pasos), desc="Ejecutando simulación"):
        print(f"\n[DEBUG] === Paso {i+1} ===")
        try:
            observacion_real = problema_confort.agente.observation_model.sample(
                problema_confort.entorno.estado_inicial, accion
            )
            
            update_belief_with_resample(problema_confort.agente, accion, observacion_real)
            
            planificador.update(problema_confort.agente, accion, observacion_real)
        except Exception as e:
            print(f"[ERROR] Fallo en la actualización: {e}")
        
        accion = safe_plan_call(planificador, problema_confort.agente)


def main():

    print("[INFO] Iniciando la simulación del problema de confort térmico.")
    try:
        confort = ProblemaConfortTermico.crear(ruido_observacion=0.15, pmv_deseado=0.0)
        if confort is None or not isinstance(confort, ProblemaConfortTermico):
            raise ValueError("[ERROR] No se generó una instancia válida de ProblemaConfortTermico.")
        
        if not hasattr(confort, "agente") or not hasattr(confort.agente, "policy"):
            raise AttributeError("[ERROR] El problema generado no tiene un agente válido o su política.")
    except Exception as e:
        print("[ERROR] Ocurrió un error al crear el problema de confort térmico:")
        traceback.print_exc()
        return

    if hasattr(confort.agente, 'get_belief'):
        belief_inicial = confort.agente.get_belief()
        logging.info(f"[DEBUG] Belief inicial: {belief_inicial}")


    try:
        planificadores = {
            "POUCT": pomdp_py.POUCT(
                max_depth=20,
                discount_factor=0.95,
                num_sims=5000,
                exploration_const=1.2,
                rollout_policy=confort.agente.policy
            ),
            "POMCP": pomdp_py.POMCP(
                max_depth=20,
                discount_factor=0.95,
                num_sims=5000,
                exploration_const=1.2,
                rollout_policy=confort.agente.policy
            )
        }
    except Exception as e:
        print("[ERROR] Falló la inicialización de los planificadores:")
        traceback.print_exc()
        return

    for nombre, planificador in planificadores.items():
        try:
            print(f"\n** Prueba de {nombre} con análisis de árbol **")
            logging.info(f"[DEBUG] Iniciando prueba para {nombre}")

            tiene_arbol = hasattr(planificador, 'tree') and planificador.tree is not None
            logging.info(f"[DEBUG] ¿Planificador {nombre} tiene árbol?: {tiene_arbol}")
            
            if hasattr(confort.agente, 'get_belief'):
                belief_antes = confort.agente.get_belief()
                logging.debug(f"[{nombre}] Belief antes de planificar: {belief_antes}")
            
            probar_planificador_con_arbol(confort, planificador, pasos=5)
            

            if hasattr(confort.agente, 'get_belief'):
                belief_despues = confort.agente.get_belief()
                logging.debug(f"[{nombre}] Belief después de planificar: {belief_despues}")
            
        except Exception as e:
            print(f"[ERROR] Falló la prueba del planificador {nombre}:")
            traceback.print_exc()

    print("[INFO] Simulación finalizada con éxito.")


if __name__ == "__main__":
    print("[INFO] Ejecutando la simulación principal...")

    logging.basicConfig(level=logging.DEBUG)
    main()
