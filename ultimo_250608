# Confort Termico c/POMDP
import pomdp_py
from pomdp_py.utils import TreeDebugger
import random
import numpy as np
import sys
import copy
import math
from itertools import product
import json
import logging
from tqdm import tqdm
from math import floor
import time
from pgmpy.models.DiscreteBayesianNetwork import DiscreteBayesianNetwork as BayesianModel
from pgmpy.factors.discrete import TabularCPD
from pgmpy.inference import VariableElimination
import traceback
from pythermalcomfort.models import pmv_ppd_iso
from pomdp_py.algorithms.po_uct import POUCT
from pomdp_py.algorithms.po_uct import RandomRollout
from scipy.stats import norm
from copy import deepcopy
from collections import Counter


def setup_logging():

    root_logger = logging.getLogger()
    if root_logger.hasHandlers():
        root_logger.handlers.clear()

    logging.basicConfig(
        filename="registro_consola.log",
        filemode="w",
        format="%(asctime)s - %(levelname)s - %(message)s",
        level=logging.DEBUG,
        force=True
    )

    class RedirigirSalida:
        def write(self, message):
            if message.strip():
                logging.info(message.strip())

        def flush(self):
            pass

    sys.stdout = RedirigirSalida()
    sys.stderr = RedirigirSalida()


setup_logging()


class EstadoConfortTermico(pomdp_py.State):
    def __init__(self, ambiente, control, persona, transferencia, pmv=None):
        super().__init__()

        if not all(isinstance(param, dict) for param in [ambiente, control, persona, transferencia]):
            raise ValueError("[ERROR] Los parámetros deben ser diccionarios válidos.")

        self.ambiente = {
            "tdb": float(ambiente["tdb"]),
            "tr": float(ambiente["tr"]),
            "rh": float(ambiente["rh"]),
            "pa": float(ambiente["Pa"])
        }
        self.control = {
            "Tt": float(control["Tt"]),
            "vr": float(control["vr"])
        }
        self.persona = {
            "met": float(persona["met"]),
            "wme": float(persona["wme"]),
            "clo": float(persona["clo"])
        }
        self.transferencia = {
            "hc": float(transferencia["hc"]),
            "Tcl": float(transferencia["Tcl"])
        }

        self.pmv = pmv if pmv is not None else self.calcular_pmv()
        if np.isnan(self.pmv) or np.isinf(self.pmv):
            logging.warning("[WARNING] PMV inicial es NaN o inf, asignando valor seguro.")
            self.pmv = 0.0  

        logging.debug(f"[DEBUG] Estado inicial creado: {self}")

    @classmethod
    def generar_estado_aleatorio(cls):
        tdb = random.uniform(15, 35)           
        tr  = np.clip(tdb + random.gauss(0, 2), 15, 40)
        ambiente = {
            "tdb": tdb,
            "tr":  tr,
            "rh": random.uniform(20, 80),
            "Pa": random.uniform(700, 4000)
        }
        control = {
            "Tt": random.uniform(18, 30),
            "vr": random.uniform(0.05, 1.5)
        }
        persona = {
            "met": random.uniform(0.7, 2.5),
            "clo": random.uniform(0.3, 1.5),
            "wme": random.uniform(0, 0.6)       
        }
        transferencia = {
            "hc": random.uniform(3.5, 6.0),
            "Tcl": random.uniform(20, 30)
        }

        estado = cls(ambiente, control, persona, transferencia)

        if np.isnan(estado.pmv) or np.isinf(estado.pmv):
            logging.warning("[WARNING] PMV inválido; asignando 0.")
            estado.pmv = 0.0
        return estado

    def calcular_pmv(self):
        tdb = self.ambiente["tdb"]
        tr  = self.ambiente["tr"]
        rh  = self.ambiente["rh"]
        vr  = self.control["vr"]
        met = self.persona["met"]
        clo = self.persona["clo"]
        wme = self.persona["wme"]

        for _ in range(20):
            try:
                return pmv_ppd_iso(tdb, tr, vr, rh, met, clo, wme).pmv
            except Exception:
                pass
        return random.uniform(-2, 2)

    def __eq__(self, other):
        return isinstance(other, EstadoConfortTermico) and \
               self.__dict__ == other.__dict__

    def __hash__(self):
        return hash((
            tuple(sorted(self.ambiente.items())),
            tuple(sorted(self.control.items())),
            tuple(sorted(self.persona.items())),
            tuple(sorted(self.transferencia.items()))
        ))


class Particula:
    def __init__(self, estado, weight=1.0):
        if isinstance(estado, Particula):
            logging.debug(
                "[DEBUG] Inicializando partícula desde otra Particula.")
            self.estado = copy.deepcopy(estado.estado)
            self.weight = estado.weight
        else:
            if not isinstance(estado, EstadoConfortTermico):
                raise TypeError(
                    "[ERROR] 'estado' debe ser una instancia de EstadoConfortTermico.")

            if not isinstance(weight, (int, float)) or weight <= 0:
                raise ValueError(
                    "[ERROR] 'weight' debe ser un número positivo.")

            self.estado = copy.deepcopy(estado)
            self.weight = float(weight)

        logging.debug(f"[DEBUG] Particula creada: estado={
                      self.estado}, weight={self.weight:.4f}")

    def copy(self):
        logging.debug("[DEBUG] Creando copia de Particula.")
        return Particula(copy.deepcopy(self.estado), self.weight)

    def resample(self, modelo_bayesiano=None):

        if self.weight < 0.05: 
            logging.info(
                f"[INFO] Resampleo activado para partícula con peso bajo ({self.weight:.4f}).")
            generador = ParticulaGenerativa(self.estado)
           
            return Particula(generador.sample(), weight=1.0)

        if modelo_bayesiano:
            evidencia = {
                "tdb": self.estado.ambiente.get("tdb", 25),
                "Tt": self.estado.control.get("Tt", 25),
                "vr": self.estado.control.get("vr", 0.5),
                "rh": self.estado.ambiente.get("rh", 50)
            }
            pmv_probabilidad = modelo_bayesiano.inferir_variable(
                "PMV", evidencia)
          
            self.weight *= max(pmv_probabilidad[1], 0.05)

        return self.copy()

    def __repr__(self):
        return f"Particula(estado={self.estado}, weight={self.weight:.4f})"


class ParticulaGenerativa(pomdp_py.framework.basics.GenerativeDistribution):

    def __init__(self, estado_base):
        if not isinstance(estado_base, EstadoConfortTermico):
            raise TypeError(
                "[ERROR] 'estado_base' debe ser una instancia de EstadoConfortTermico.")
        self.estado_base = estado_base

    def sample(self):
        return EstadoConfortTermico(
            ambiente={
                "tdb": np.clip(self.estado_base.ambiente["tdb"] + np.random.normal(0, 1.5), 10, 35),
                "tr": np.clip(self.estado_base.ambiente["tr"] + np.random.normal(0, 1.2), 18, 30),
                "rh": np.clip(self.estado_base.ambiente["rh"] + np.random.normal(0, 2.0), 20, 80),
                "pa": self.estado_base.ambiente["pa"]
            },
            control={
                "Tt": np.clip(self.estado_base.control["Tt"] + np.random.normal(0, 1.0), 16, 33),
                "vr": np.clip(self.estado_base.control["vr"] + np.random.normal(0, 0.3), 0.05, 2.0)
            },
            persona={
                "met": np.clip(self.estado_base.persona["met"] + np.random.normal(0, 0.02), 0.8, 1.8),
                "wme": np.clip(self.estado_base.persona["wme"] + np.random.normal(0, 0.01), 0.1, 0.3),
                "clo": np.clip(self.estado_base.persona["clo"] + np.random.normal(0, 0.02), 0.5, 1.2)
            },
            transferencia={
                "hc": np.clip(self.estado_base.transferencia["hc"] + np.random.normal(0, 0.5), 2, 6),
                "Tcl": np.clip(self.estado_base.transferencia["Tcl"] + np.random.normal(0, 0.8), 22, 30)
            },
            pmv=None
        )

    def mpe(self):
        return self.estado_base

    def get_histogram(self):
        return {self.sample(): 1.0}  


class ObservacionConfort(pomdp_py.Observation):

    def __init__(self, tdb, Tt, vr, rh):
        self.tdb = float(np.clip(tdb if tdb is not None else 25.0, 10.0, 35.0))
        self.Tt = float(np.clip(Tt if Tt is not None else 22.0, 16.0, 33.0))
        self.vr = float(np.clip(vr if vr is not None else 0.5, 0.05, 2.0))
        self.rh = float(np.clip(rh if rh is not None else 50.0, 20.0, 80.0))

        logging.debug(f"[DEBUG] Observación inicializada: tdb={
                      self.tdb}, Tt={self.Tt}, vr={self.vr}, rh={self.rh}")

    def simular_variacion(self):
        tdb_variacion = np.random.uniform(-0.5, 0.5)
        rh_variacion = np.random.uniform(-1, 1)
        Tt_variacion = np.random.uniform(-0.5, 0.5)
        vr_variacion = np.random.uniform(-0.05, 0.05)

        obs_variada = ObservacionConfort(
            tdb=np.clip(self.tdb + tdb_variacion, 10.0, 35.0),
            Tt=np.clip(self.Tt + Tt_variacion, 16.0, 33.0),
            vr=np.clip(self.vr + vr_variacion, 0.05, 2.0),
            rh=np.clip(self.rh + rh_variacion, 20.0, 80.0)
        )

        logging.info(f"[DEBUG] Observación variada generada: {obs_variada}")
        return obs_variada

    def __repr__(self):
        return f"ObservacionConfort(tdb={self.tdb:.2f}, Tt={self.Tt:.2f}, vr={self.vr:.2f}, rh={self.rh:.2f})"

    def __eq__(self, other):
        return (
            isinstance(other, ObservacionConfort)
            and math.isclose(self.tdb, other.tdb, abs_tol=1e-5)
            and math.isclose(self.Tt, other.Tt, abs_tol=1e-5)
            and math.isclose(self.vr, other.vr, abs_tol=1e-5)
            and math.isclose(self.rh, other.rh, abs_tol=1e-5)
        )

    def __hash__(self):
        hash_val = hash(tuple(round(x, 2)
                        for x in (self.tdb, self.Tt, self.vr, self.rh)))
        logging.debug(f"[DEBUG] Hash de observación: {hash_val}")
        return hash_val


class ModeloDeObservacion(pomdp_py.ObservationModel):

    def __init__(self, ruido_observacion=0.15):
        if not isinstance(ruido_observacion, (float, int)) or ruido_observacion < 0:
            raise ValueError("[ERROR] 'ruido_observacion' debe ser un número positivo.")

        self.ruido_observacion = max(0.5, min(10, ruido_observacion * 2))
        logging.info(f"[DEBUG] ModeloDeObservacion inicializado con ruido_observacion={self.ruido_observacion}")

    def sample(self, estado, accion):
        try:
            if not isinstance(estado, EstadoConfortTermico):
                raise TypeError("[ERROR] 'estado' debe ser una instancia de EstadoConfortTermico.")

            observacion = ObservacionConfort(
                tdb=np.clip(estado.ambiente.get("tdb", 25) + np.random.normal(0, self.ruido_observacion), 10, 35),
                Tt=np.clip(estado.control.get("Tt", 25) + np.random.normal(0, self.ruido_observacion), 16, 33),
                vr=np.clip(estado.control.get("vr", 0.5) + np.random.normal(0, self.ruido_observacion), 0.05, 2.0),
                rh=np.clip(estado.ambiente.get("rh", 50) + np.random.normal(0, self.ruido_observacion), 20, 80)
            )

            logging.debug(f"[DEBUG] Observación generada correctamente: {observacion}")
            return observacion

        except Exception as e:
            logging.error(f"[ERROR] Falló la generación de la observación: {e}")
            raise e

    def probability(self, obs, estado, accion):
        try:
            if not isinstance(obs, ObservacionConfort):
                raise TypeError("[ERROR] 'obs' debe ser una instancia de ObservacionConfort.")
            if not isinstance(estado, EstadoConfortTermico):
                raise TypeError("[ERROR] 'estado' debe ser una instancia de EstadoConfortTermico.")

            sigma = max(15.0, min(100, self.ruido_observacion * 10))
        
            state_values = {
                "tdb": estado.ambiente.get("tdb", 25),
                "Tt": estado.control.get("Tt", 25),
                "vr": estado.control.get("vr", 0.5),
                "rh": estado.ambiente.get("rh", 50)
            }
        
            attribute_weights = {
                "tdb": 0.02,
                "Tt": 0.02,
                "vr": 0.01,
                "rh": 0.005  
            }
        
            log_components = []
            for attr in ["tdb", "Tt", "vr", "rh"]:
                diff = getattr(obs, attr) - state_values[attr]
                diff = np.clip(diff, -5, 5)
            
                comp = attribute_weights[attr] * (
                    -0.5 * (diff / sigma) ** 2 - np.log(sigma * np.sqrt(2 * np.pi))
                )
                log_components.append(comp)
                logging.debug(f"[DEBUG] Atributo '{attr}': diff = {diff}, componente = {comp}")
        
            log_prob = np.sum(log_components)
            logging.debug(f"[DEBUG] Suma total de log_prob: {log_prob}")
        
            probabilidad = np.exp(log_prob)
            logging.debug(f"[DEBUG] Probabilidad calculada correctamente: {probabilidad}")
            return probabilidad

        except Exception as e:
            logging.error(f"[ERROR] Falló el cálculo de probabilidad: {e}")
            raise e

    def get_all_observations(self):
        try:
            observaciones = [
                ObservacionConfort(
                    tdb=np.clip(np.random.normal(25, 5), 10, 35),
                    Tt=np.clip(np.random.normal(24, 4), 16, 33),
                    vr=np.clip(np.random.normal(0.5, 0.2), 0.05, 2.0),
                    rh=np.clip(np.random.normal(50, 15), 20, 80)
                )
                for _ in range(100)
            ]

            logging.debug(f"[DEBUG] Se generaron {len(observaciones)} observaciones posibles.")
            return observaciones

        except Exception as e:
            logging.error(f"[ERROR] Falló la generación de observaciones: {e}")
            raise e


class ModeloBayesianoConfort:
    def __init__(self):

        self.model = BayesianModel([
            ('Tt', 'tdb'),
            ('Tt', 'tr'),
            ('tdb', 'Pa'),
            ('rh', 'Pa'),
            ('Tt', 'Tcl'),
            ('Tt', 'PMV'),
            ('vr', 'PMV'),
            ('rh', 'PMV')
        ])
        
        self.infer = None

        try:
            self.construir_red()
        except Exception as e:
            logging.error(f"[ERROR] No se pudo construir la red Bayesiana: {e}")
            self.infer = None 

    def construir_red(self):
        try:

            cpd_Tt = TabularCPD(variable='Tt', variable_card=2, values=[[0.3], [0.7]])
            cpd_vr = TabularCPD(variable='vr', variable_card=2, values=[[0.5], [0.5]])
            cpd_rh = TabularCPD(variable='rh', variable_card=2, values=[[0.6], [0.4]])
         
            cpd_tdb = TabularCPD(
                variable='tdb', variable_card=2,
                values=[[1.0, 0.0],
                        [0.0, 1.0]],
                evidence=['Tt'], evidence_card=[2]
            )
            cpd_tr = TabularCPD(
                variable='tr', variable_card=2,
                values=[[0.8, 0.4],
                        [0.2, 0.6]],
                evidence=['Tt'], evidence_card=[2]
            )
            cpd_Pa = TabularCPD(
                variable='Pa', variable_card=2,
                values=[[0.7, 0.5, 0.6, 0.3],
                        [0.3, 0.5, 0.4, 0.7]],
                evidence=['tdb', 'rh'], evidence_card=[2, 2]
            )
            cpd_Tcl = TabularCPD(
                variable='Tcl', variable_card=2,
                values=[[0.7, 0.4],
                        [0.3, 0.6]],
                evidence=['Tt'], evidence_card=[2]
            )

            values_pmv = np.full((2, 8), 0.5).tolist()
            logging.debug(f"[DEBUG] Forma generada para `cpd_PMV`: {np.array(values_pmv).shape}")
            cpd_PMV = TabularCPD(
                variable='PMV', variable_card=2,
                values=values_pmv,
                evidence=['Tt', 'vr', 'rh'],
                evidence_card=[2, 2, 2]
            )
            if np.array(values_pmv).shape != (2, 8):
                raise ValueError(f"[ERROR] Forma de `cpd_PMV` incorrecta: {np.array(values_pmv).shape}. Debe ser (2,8).")
            
            self.model.add_cpds(
                cpd_Tt, cpd_vr, cpd_rh,
                cpd_tdb, cpd_tr, cpd_Pa, cpd_Tcl, cpd_PMV
            )
            
            if not self.model.check_model():
                raise Exception("[ERROR] El modelo Bayesiano no es válido.")
            
            self.infer = VariableElimination(self.model)
            logging.info("[INFO] Modelo Bayesiano configurado correctamente.")
        except Exception as e:
            logging.error(f"[ERROR] No se pudo inicializar el modelo bayesiano: {e}")
            raise

    def inferir_pmv(self, evidencia):

        try:
            if not isinstance(evidencia, dict):
                raise TypeError("[ERROR] La evidencia debe ser un diccionario.")
            if self.infer is None:
                raise ValueError("[ERROR] La inferencia no está inicializada correctamente. Verifica construir_red().")
            
            query_result = self.infer.query(variables=['PMV'], evidence=evidencia)
            return query_result['PMV']
        except Exception as e:
            logging.error(f"[ERROR] Fallo en la inferencia de PMV: {e}")
            return None


class ModeloDeTransicion(pomdp_py.TransitionModel):
    def __init__(self, estados=None):
        super().__init__()
        logging.info("[DEBUG] Se ha creado una instancia de ModeloDeTransicion")
        self.estados = estados if estados else []

        try:
            self.modelo_bayesiano = ModeloBayesianoConfort()
            self.modelo_bayesiano.construir_red()
            logging.info("[DEBUG] Modelo bayesiano inicializado correctamente.")
        except Exception as e:
            logging.error(f"[ERROR] No se pudo inicializar el modelo bayesiano: {e}")
            self.modelo_bayesiano = None

    def probability(self, estado_siguiente, estado_actual, accion):

        try:
            logging.info("[DEBUG] Entré en probability")

            def compute_soft_evidence(value, threshold, sigma):
                p0 = np.exp(-0.5 * ((value - threshold) / sigma) ** 2)
                p1 = 1 - p0
                norm = max(p0 + p1, 1.0) 
                return [p0 / norm, p1 / norm]

            thresholds = {"tdb": 25, "Tt": 25, "vr": 0.5, "rh": 50}
            sigma_vals = {"tdb": 2.0, "Tt": 2.0, "vr": 0.1, "rh": 10.0}
            keys = ["tdb", "Tt", "vr", "rh"]

            soft_evidence = {
                key: compute_soft_evidence(
                    estado_actual.ambiente.get(key, thresholds[key]),
                    thresholds[key],
                    sigma_vals[key]
                )
                for key in keys
            }
            logging.debug(f"[DEBUG] Soft evidence calculada: {soft_evidence}")

            total_prob = 0.0

            for combination in product([0, 1], repeat=len(keys)):
                weight = np.prod([soft_evidence[key][combination[i]]
                                  for i, key in enumerate(keys)])
                evidence = {keys[i]: combination[i] for i in range(len(keys))}
                logging.debug(f"[DEBUG] Evidencia: {evidence}, Peso parcial: {weight:.8f}")

                pmv_distribution = self.modelo_bayesiano.inferir_pmv(evidence) if self.modelo_bayesiano else None

                if pmv_distribution is None:
                    logging.error("Inferencia de PMV devolvió None para la evidencia: " + str(evidence))
                    continue

                if not hasattr(pmv_distribution, 'values') or pmv_distribution.values is None:
                    logging.error("El resultado de la inferencia no posee un atributo 'values' válido. Evidencia: " + str(evidence))
                    continue


                estado_siguiente.pmv = estado_siguiente.calcular_pmv()
                observed_state = 0 if estado_siguiente.pmv < 0 else 1

                obtained_val = None
                try:
                    obtained_val = pmv_distribution.get_value(PMV=observed_state)
                except Exception as ex:
                    logging.error(f"[ERROR] Falló get_value(): {ex}")

                if obtained_val is None:
                    if hasattr(pmv_distribution, "values") and pmv_distribution.values is not None:
                        try:
                            obtained_val = pmv_distribution.values[observed_state]
                        except Exception as ex2:
                            logging.error(f"[ERROR] Fallo al indexar pmv_distribution.values: {ex2}")
                    else:
                        logging.error("pmv_distribution no tiene un atributo 'values' válido. Evidencia: " + str(evidence))

                if obtained_val is None:
                    continue

                contrib = weight * obtained_val
                logging.debug(f"[DEBUG] Contribución calculada: {contrib:.8f}")
                total_prob += contrib

            logging.info(f"[DEBUG] Probabilidad total calculada: {total_prob:.8f}")
            return total_prob

        except Exception as e:
            logging.error(f"[ERROR] Falló la inferencia bayesiana en transición con soft evidence: {e}")
            return 0.0


class ModeloDeRecompensa(pomdp_py.RewardModel):

    def __init__(self, pmv_deseado=0.0, factor_pmv=1.0):
        super().__init__()

        if not isinstance(pmv_deseado, (int, float)):
            raise ValueError("[ERROR] 'pmv_deseado' debe ser un número (int o float).")
        if not isinstance(factor_pmv, (int, float)) or factor_pmv <= 0:
            raise ValueError("[ERROR] 'factor_pmv' debe ser un número positivo.")

        self.pmv_deseado = pmv_deseado
        self.factor_pmv = factor_pmv

    def reward(self, estado, accion):
        try:
            pmv_actual = estado.pmv if estado.pmv is not None else estado.calcular_pmv()
            logging.debug(f"[DEBUG] Evaluando recompensa: PMV={pmv_actual}, Acción={accion}")

            desviacion_pmv = abs(pmv_actual - self.pmv_deseado)

            if desviacion_pmv < 0.5: 
                return 10.0 * self.factor_pmv
            elif desviacion_pmv < 1.5: 
                return 5.0 * self.factor_pmv
            else: 
                return -10.0 * self.factor_pmv

        except Exception as e:
            logging.error(f"[ERROR] Fallo en cálculo de recompensa: {e}")
            return -20.0 


class ModeloDePolitica(pomdp_py.RolloutPolicy):

    def __init__(self, pmv_deseado, acciones=None):
        super().__init__()

        if not isinstance(pmv_deseado, (int, float)):
            raise ValueError("[ERROR] 'pmv_deseado' debe ser un número (int o float).")

        self.pmv_deseado = pmv_deseado

        if acciones is None or not (isinstance(acciones, list) and acciones):
            self.acciones = [
                AccionConfortTermico(cambio_de_temperatura=-0.5, cambio_de_flujo_de_aire=0.0),
                AccionConfortTermico(cambio_de_temperatura=0.0, cambio_de_flujo_de_aire=0.0),
                AccionConfortTermico(cambio_de_temperatura=0.0, cambio_de_flujo_de_aire=0.5)
            ]
        else:
            self.acciones = [accion if isinstance(accion, AccionConfortTermico)
                             else AccionConfortTermico(*accion)
                             for accion in acciones]

    def get_all_actions(self, state=None, history=None):

        return self.acciones

    def rollout(self, estado, history=None):
        try:
            logging.debug(f"[DEBUG] Rollout ejecutado para estado {estado}")
            if estado.pmv is not None and estado.pmv > self.pmv_deseado + 1.5:
                accion = AccionConfortTermico(cambio_de_temperatura=-0.5, cambio_de_flujo_de_aire=0.0)
            elif estado.pmv is not None and estado.pmv < self.pmv_deseado - 0.5:
                accion = AccionConfortTermico(cambio_de_temperatura=0.0, cambio_de_flujo_de_aire=0.5)
            else:
                accion = AccionConfortTermico(cambio_de_temperatura=0.0, cambio_de_flujo_de_aire=0.0)
            return accion

        except Exception as e:
            logging.error(f"[ERROR] Fallo en la política de rollout: {e}")
            return AccionConfortTermico(cambio_de_temperatura=0.0, cambio_de_flujo_de_aire=0.0)


class AccionConfortTermico(pomdp_py.Action):

    def __init__(self, cambio_de_temperatura: float = 0.0, cambio_de_flujo_de_aire: float = 0.0):
        if not isinstance(cambio_de_temperatura, (int, float)):
            raise TypeError("[ERROR] 'cambio_de_temperatura' debe ser un número.")
        if not isinstance(cambio_de_flujo_de_aire, (int, float)):
            raise TypeError("[ERROR] 'cambio_de_flujo_de_aire' debe ser un número.")
        if not (-1.0 <= cambio_de_temperatura <= 1.0):
            raise ValueError(f"[ERROR] Valor fuera de rango: cambio_de_temperatura={cambio_de_temperatura}")
        if not (-1.0 <= cambio_de_flujo_de_aire <= 1.0):
            raise ValueError(f"[ERROR] Valor fuera de rango: cambio_de_flujo_de_aire={cambio_de_flujo_de_aire}")

        self.cambio_de_temperatura = round(cambio_de_temperatura, 2)
        self.cambio_de_flujo_de_aire = round(cambio_de_flujo_de_aire, 2)

    def __repr__(self):
        return (f"AccionConfortTermico(Temp={self.cambio_de_temperatura}, "
                f"Flujo={self.cambio_de_flujo_de_aire})")

    def __eq__(self, other):
        return (isinstance(other, AccionConfortTermico) and 
                self.cambio_de_temperatura == other.cambio_de_temperatura and 
                self.cambio_de_flujo_de_aire == other.cambio_de_flujo_de_aire)

    def __hash__(self):
        return hash((self.cambio_de_temperatura, self.cambio_de_flujo_de_aire))

    def ejecutar_accion(self, estado):
        estado.ambiente["tdb"] += self.cambio_de_temperatura
        estado.control["vr"] += self.cambio_de_flujo_de_aire

        estado.ambiente["tdb"] = max(2.0, min(35.0, estado.ambiente["tdb"]))
        estado.control["vr"] = max(0.0, min(2.0, estado.control["vr"]))

        estado.pmv = estado.calcular_pmv()
        logging.debug(f"[DEBUG] Acción aplicada: tdb={estado.ambiente['tdb']}, vr={estado.control['vr']}")

    def es_accion_valida(self) -> bool:
        return -1.0 <= self.cambio_de_temperatura <= 1.0 and -1.0 <= self.cambio_de_flujo_de_aire <= 1.0


class ModeloDeEnvironment(pomdp_py.Environment):

    def __init__(self, modelo_transicion, modelo_observacion, modelo_recompensa, estado_inicial):
        super().__init__(modelo_transicion, modelo_observacion,
                         modelo_recompensa, estado_inicial)

        if not isinstance(estado_inicial, EstadoConfortTermico):
            raise TypeError(
                "[ERROR] 'estado_inicial' debe ser una instancia de EstadoConfortTermico.")

        logging.info("[INFO] Entorno inicializado correctamente.")

    def step(self, accion):
        try:
            logging.debug(f"[DEBUG] Ejecutando acción: {accion}")

            estado_siguiente = self.transition_model.sample(self.state, accion)
            observacion = self.observation_model.sample(
                estado_siguiente, accion)
            recompensa = self.reward_model.reward(estado_siguiente, accion)

            self.state = estado_siguiente 

            return observacion, recompensa, estado_siguiente

        except Exception as e:
            logging.error(f"[ERROR] Fallo en step(): {e}")
            return None, -20.0, self.state 


class AgentePersonalizado(pomdp_py.Agent):

    def __init__(self, belief, policy, transition_model, observation_model, reward_model, pmv, nombre=None):
        self.nombre = nombre or "AgenteDesconocido"

        try:
            if not isinstance(belief, pomdp_py.Particles):
                raise TypeError(
                    "[ERROR] 'belief' debe ser una instancia de pomdp_py.Particles.")

            if not isinstance(policy, ModeloDePolitica):
                raise TypeError(
                    "[ERROR] 'policy' debe ser una instancia de ModeloDePolitica.")

            if not all([transition_model, observation_model, reward_model]):
                raise ValueError(
                    "[ERROR] Uno o más modelos (transición, observación, recompensa) son None.")

            self.policy = policy
            super().__init__(belief, policy, transition_model, observation_model, reward_model)

            self.pmv = pmv

            self.rollout_policy = pomdp_py.RandomRollout()
            self.planificador = pomdp_py.POUCT(
                max_depth=10,
                num_sims=500,
                discount_factor=0.95,
                rollout_policy=self.rollout_policy
            )

            logging.info(
                f"[INFO] AgentePersonalizado inicializado con éxito: {self.nombre}")

        except Exception as e:
            logging.exception(
                "[ERROR] Falló la inicialización de AgentePersonalizado:")
            raise e

    def plan(self):
        try:
            accion = super().plan()

            if accion is None: 
                logging.warning(
                    "[WARN] Acción obtenida es None, intentando recuperación.")
                accion = self.recuperar_accion_defecto()

        except Exception as e:
            logging.error(
                "[ERROR] Error en plan(), activando recuperación de acción:", exc_info=True)
            accion = self.recuperar_accion_defecto()

        return accion

    def recuperar_accion_defecto(self):
        try:
            acciones_disponibles = self.policy.get_all_actions()
            if acciones_disponibles:
                logging.info(
                    "[INFO] Se usa la primera acción disponible como acción por defecto.")
                return acciones_disponibles[0]
            else:
                logging.error(
                    "[ERROR] La política no tiene acciones disponibles. No se puede definir acción por defecto.")
                return None
        except Exception as e:
            logging.error(
                "[ERROR] Fallo al obtener acciones de la política:", exc_info=True)
            return None

    def actualizar_belief(self, observacion):
        try:
            nueva_creencia = self.policy.update_belief(
                self.belief, observacion)
            self.belief = nueva_creencia
            logging.info(
                "[INFO] Creencia del agente actualizada correctamente.")
        except Exception as e:
            logging.error(
                "[ERROR] Fallo en la actualización de creencias:", exc_info=True)


class EntornoConfort(pomdp_py.Environment):

    def __init__(self, estados, estado_inicial):
        if not isinstance(estados, (list, set)) or not all(isinstance(e, EstadoConfortTermico) for e in estados):
            raise ValueError(
                "[ERROR] 'estados' debe ser una lista o conjunto de instancias de EstadoConfortTermico.")
        if not isinstance(estado_inicial, EstadoConfortTermico):
            raise ValueError(
                "[ERROR] 'estado_inicial' debe ser una instancia válida de EstadoConfortTermico.")

        self.estados = copy.deepcopy(estados)
        self.estado_inicial = estado_inicial

        super().__init__(estado_inicial)
        logging.debug(f"[DEBUG] EntornoConfort inicializado con {
                      len(self.estados)} estados posibles.")

    def estados_posibles(self):
        return self.estados

    def step(self, accion):
        try:
            estado_siguiente = self.transition_model.sample(self.state, accion)
            observacion = self.observation_model.sample(
                estado_siguiente, accion)
            recompensa = self.reward_model.reward(estado_siguiente, accion)

            self.state = estado_siguiente 

            return observacion, recompensa, estado_siguiente

        except Exception as e:
            logging.error(f"[ERROR] Fallo en step(): {e}")
            return None, -20.0, self.state 

    def __repr__(self):
        estados_ejemplo = self.estados[:3] if len(
            self.estados) > 3 else self.estados
        return f"EntornoConfort(estado_inicial={self.estado_inicial}, estados={len(self.estados)}, ejemplo={estados_ejemplo})"


def crear_creencia_inicial_en_particulas(num_particulas=1000):
    
    try:
        logging.info(f"[INFO] Creando creencia inicial con {num_particulas} partículas...")

        particulas = [Particula(EstadoConfortTermico.generar_estado_aleatorio(), weight=1.0)
                      for _ in range(num_particulas)]

        particulas_validas = [p for p in particulas if not np.isnan(p.estado.pmv) and not np.isinf(p.estado.pmv)]
        cantidad_faltante = num_particulas - len(particulas_validas)

        if cantidad_faltante > 0:
            logging.warning(f"[WARNING] {cantidad_faltante} partículas inválidas eliminadas, generando reemplazos...")
            nuevas_particulas = [Particula(EstadoConfortTermico.generar_estado_aleatorio(), weight=1.0)
                                 for _ in range(cantidad_faltante)]
            particulas_validas.extend(nuevas_particulas)

        total_peso = sum(p.weight for p in particulas_validas) or 1.0
        for p in particulas_validas:
            p.weight /= total_peso 

        belief = pomdp_py.Particles(particulas_validas)
        logging.info("[INFO] Creencia inicial generada correctamente con partículas validadas.")

        return belief

    except Exception as e:
        logging.error("[ERROR] Fallo en la generación de la creencia inicial:", exc_info=True)
        return None


def validate_estado(estado):
    estado.ambiente["tdb"] = np.clip(estado.ambiente.get("tdb", 25), 15, 30)
    estado.ambiente["tr"] = np.clip(estado.ambiente.get("tr", 25), 15, 35)
    estado.ambiente["rh"] = np.clip(estado.ambiente.get("rh", 50), 20, 90)
    estado.ambiente["Pa"] = np.clip(estado.ambiente.get("Pa", 1000), 800, 1200)
    estado.control["Tt"] = np.clip(estado.control.get("Tt", 25), 16, 33)
    estado.control["vr"] = np.clip(estado.control.get("vr", 0.5), 0, 2)
    return estado


def observacion_a_vector(observacion):
    orden = ["tdb", "tr", "rh", "Pa", "Tt"]
    if isinstance(observacion, dict):
        return np.array([observacion.get(clave, 0) for clave in orden])
    elif hasattr(observacion, '__dict__'):
        return np.array([getattr(observacion, clave, 0) for clave in orden])
    else:
        raise TypeError("[ERROR] Tipo de observación no soportado.")


def convert_belief_to_state_list(belief):
    return [getattr(p, 'estado', p) for p in belief.particles]


def update_belief_with_resample(agente, accion, observacion_real):

    if not hasattr(agente, "policy") or not hasattr(agente, "cur_belief"):
        logging.error("[ERROR] El agente no tiene una política o creencia válida.")
        return

    particulas_seguras = copy.deepcopy(agente.cur_belief.particles)
    particulas_validas = [p for p in particulas_seguras if isinstance(p, Particula) and not np.isnan(p.estado.pmv)]
    cantidad_faltante = len(particulas_seguras) - len(particulas_validas)
    if cantidad_faltante > 0:
        logging.warning(f"[WARNING] {cantidad_faltante} partículas inválidas eliminadas y reemplazadas.")
        nuevas_particulas = [Particula(EstadoConfortTermico.generar_estado_aleatorio())
                              for _ in range(cantidad_faltante)]
        particulas_validas += nuevas_particulas

    particulas_finales = particulas_validas[:]
    particulas_finales = roughen_particles(particulas_finales) 

    for p in particulas_finales:
        p.estado.control["Tt"] = p.estado.control.get("Tt", round(np.random.uniform(16, 33), 0))

    agente.set_belief(pomdp_py.Particles(copy.deepcopy(particulas_finales)))
    modelo_obs = ModeloDeObservacion()

    for p in particulas_finales:
        p.weight = modelo_obs.probability(observacion_real, p.estado, accion)

    probs = [p.weight for p in particulas_finales]
    log_probs = [np.log(prob) if prob > 0 else -np.inf for prob in probs]
    max_log = max(log_probs)
    exp_weights = [np.exp(lp - max_log) if lp != -np.inf else 0.0 for lp in log_probs]
    sum_exp = sum(exp_weights)
    normalized_weights = [w / sum_exp for w in exp_weights]
    for i, p in enumerate(particulas_finales):
        p.weight = normalized_weights[i]

    EPS = 1e-6
    weights = np.array([p.weight for p in particulas_finales])
    weights = np.maximum(weights, EPS)
    weights /= weights.sum()
    
    logging.debug(f"[TRACE] Peso mínimo = {weights.min()}, Peso máximo = {weights.max()}")

    pmvs = [p.estado.pmv for p in particulas_finales]
    logging.debug(f"[TRACE] PMV min={min(pmvs):.2f} max={max(pmvs):.2f} std={np.std(pmvs):.2f}")

    tts  = [p.estado.control["Tt"] for p in particulas_finales]
    tdbs = [p.estado.ambiente["tdb"] for p in particulas_finales]
    vrs  = [p.estado.control["vr"] for p in particulas_finales]
    rhs  = [p.estado.ambiente["rh"] for p in particulas_finales]
    clos = [p.estado.persona["clo"] for p in particulas_finales]

    logging.debug(f"[TRACE] Tt min={min(tts):.2f} max={max(tts):.2f} std={np.std(tts):.2f}")
    logging.debug(f"[TRACE] tdb min={min(tdbs):.2f} max={max(tdbs):.2f} std={np.std(tdbs):.2f}")
    logging.debug(f"[TRACE] vr min={min(vrs):.2f} max={max(vrs):.2f} std={np.std(vrs):.2f}")
    logging.debug(f"[TRACE] rh min={min(rhs):.2f} max={max(rhs):.2f} std={np.std(rhs):.2f}")
    logging.debug(f"[TRACE] clo min={min(clos):.2f} max={max(clos):.2f} std={np.std(clos):.2f}")

    low_dispersion_thresholds = {
        "Tt": 0.5,
        "tdb": 1.0,
        "vr": 0.1,
        "rh": 2.0,
        "clo": 0.2
    }

    if np.std(tts) < low_dispersion_thresholds["Tt"] or \
       np.std(tdbs) < low_dispersion_thresholds["tdb"] or \
       np.std(vrs) < low_dispersion_thresholds["vr"] or \
       np.std(rhs) < low_dispersion_thresholds["rh"] or \
       np.std(clos) < low_dispersion_thresholds["clo"]:
        logging.warning("Baja dispersión en múltiples variables; inyectando más partículas.")
        extra_ratio = 0.10  
        n_extra = max(1, int(extra_ratio * len(particulas_finales)))
        logging.debug(f"[TRACE] Inyectando {n_extra} partículas nuevas adicionales por baja dispersión.")
        extra_particles = [Particula(EstadoConfortTermico.generar_estado_aleatorio())
                           for _ in range(n_extra)]
        particulas_finales.extend(extra_particles)
        total = len(particulas_finales)
        for p in particulas_finales:
            p.weight = 1.0 / total

    desired_N = 1000
    if len(particulas_finales) < desired_N:
        particulas_finales.extend([Particula(EstadoConfortTermico.generar_estado_aleatorio())
                                   for _ in range(desired_N - len(particulas_finales))])
    final_particles = particulas_finales
    for p in final_particles:
        p.weight = 1.0 / desired_N

    agente.set_belief(pomdp_py.Particles(final_particles))
    logging.info(f"[DEBUG] Creencia final re-sampleada con {desired_N} partículas.")

def agregar_jitter(particulas, sigma_tdb=1.0, sigma_rh=5, sigma_Tt=2.0, sigma_vr=0.2):
    for p in particulas:
        p.estado.ambiente["tdb"] += np.random.normal(0, sigma_tdb)
        p.estado.ambiente["tdb"] = np.clip(p.estado.ambiente["tdb"], 10, 35)
        p.estado.ambiente["rh"]  += np.random.normal(0, sigma_rh)
        p.estado.ambiente["rh"]  = np.clip(p.estado.ambiente["rh"], 20, 80)
        p.estado.control["Tt"]   += np.random.normal(0, sigma_Tt)
        p.estado.control["Tt"]   = np.clip(p.estado.control["Tt"], 16, 33)
        p.estado.control["vr"]   += np.random.normal(0, sigma_vr)
        p.estado.control["vr"]   = np.clip(p.estado.control["vr"], 0.05, 2.0)
        p.estado.persona["met"] += np.random.normal(0, 0.4)
        p.estado.persona["met"] = np.clip(p.estado.persona["met"], 0.7, 2.5)
        p.estado.persona["clo"] += np.random.normal(0, 0.2)
        p.estado.persona["clo"] = np.clip(p.estado.persona["clo"], 0.3, 1.5)
        p.estado.persona["wme"] += np.random.normal(0, 0.1)
        p.estado.persona["wme"] = np.clip(p.estado.persona["wme"], 0.0, 0.6)

        p.estado.pmv = p.estado.calcular_pmv()


def calculate_n_eff(particles):
    weights = np.array([p.weight for p in particles])
    return 1.0 / np.sum(np.square(weights)) if np.sum(np.square(weights)) > 0 else 0


def systematic_resample(particles):
    N = len(particles)
    positions = (np.arange(N) + random.random()) / N
    cumulative_sum = np.cumsum([p.weight for p in particles])
    cumulative_sum[-1] = 1.0
    indexes = np.searchsorted(cumulative_sum, positions)
    new_particles = [particles[int(i)].copy() for i in indexes]
    for p in new_particles:
        p.weight = 1.0 / N
    return new_particles


def RS_resample(N, weights, particles):
    cumulative_sum = np.cumsum(weights)
    positions = np.random.uniform(0, 1.0 / N) + np.arange(N) / N
    indexes = np.searchsorted(cumulative_sum, positions)
    indexes = [min(idx, len(particles) - 1) for idx in indexes]
    new_particles = [particles[int(i)].copy() for i in indexes]
    for p in new_particles:
        p.weight = 1.0 / N
    return new_particles


def roughen_particles(particles, sigma_factor=0.8, jitter_min=0.1):
    for p in particles:
        p.estado.ambiente["tdb"] += np.random.normal(
            0, max(sigma_factor, jitter_min))
        p.estado.ambiente["tr"] += np.random.normal(
            0, max(sigma_factor, jitter_min))
        p.estado.control["vr"] += np.random.normal(
            0, max(sigma_factor, jitter_min))
        p.estado.control["Tt"] += np.random.normal(
            0, max(sigma_factor, jitter_min))
    return particles


def perturb_weights(particles, min_factor=0.95, max_factor=1.05):
    for p in particles:
        p.weight *= np.random.uniform(min_factor, max_factor)
    total_weight = sum(p.weight for p in particles) + 1e-6
    for p in particles:
        p.weight /= total_weight
    return particles


def update_particles(particles, n_eff_threshold):
    n_eff = calculate_n_eff(particles)
    logging.debug(f"[DEBUG] n_eff calculado: {n_eff}")
    if n_eff < n_eff_threshold:
        logging.info(f"[DEBUG] n_eff {n_eff} menor al umbral {
                     n_eff_threshold}. Re-muestreando...")
        particles = systematic_resample(particles)
    return particles


def prueba_basica():

    logging.info("[INFO] Ejecutando prueba básica del sistema POMDP.")

    try:
        ambiente = {"rh": 38.0, "Pa": 93.9, "tdb": 24, "tr": 25.3}
        control = {"Tt": 20, "vr": 0.75}
        persona = {"met": 1.05, "wme": 0.18, "clo": 1.86}
        transferencia = {"hc": 0.92, "Tcl": 22.4}
        pmv = 1.0

        logging.debug("[DEBUG] Inicializando EstadoConfortTermico...")
        try:
            estado_inicial = EstadoConfortTermico(
                ambiente, control, persona, transferencia, pmv)
            logging.debug(f"[DEBUG] Estado inicial creado correctamente: {
                          estado_inicial}")
        except Exception as e:
            logging.error(
                f"[ERROR] Fallo en la inicialización de EstadoConfortTermico: {e}")
            return

        modelo_transicion = ModeloDeTransicion()
        logging.debug(f"[DEBUG] Modelo de Transición creado: {
                      modelo_transicion}")

        try:
            logging.info(
                "[TEST] Probando `crear_creencia_inicial_en_particulas()`...")
            creencia = crear_creencia_inicial_en_particulas(
                num_particulas=1000)
            logging.info(
                f"[TEST] Creencia inicial creada con éxito: {creencia}")

            pesos = [p.weight for p in creencia.particles]
            logging.debug(f"[DEBUG] Rango de pesos en creencia inicial: min={
                          min(pesos)}, max={max(pesos)}, promedio={sum(pesos) / len(pesos)}")

            if min(pesos) < 0.5:
                logging.warning(
                    "[WARNING] Algunos pesos son demasiado bajos. Ajustando...")
                for p in creencia.particles:
                    p.weight = max(0.5, p.weight)

        except Exception as e:
            logging.error(f"[TEST ERROR] La prueba de creencia inicial falló: {
                          type(e).__name__}: {e}")
            return

        if not isinstance(ModeloDePolitica(), pomdp_py.Policy):
            logging.error(
                "[ERROR] 'ModeloDePolitica' no es una política válida.")
            return

        try:
            pouct = pomdp_py.POUCT(
                max_depth=5,
                discount_factor=0.95,
                num_sims=100,
                exploration_const=1.0,
                rollout_policy=ModeloDePolitica()
            )
            logging.debug(f"[DEBUG] Planificador POUCT configurado: {pouct}")
        except Exception as e:
            logging.error(f"[ERROR] Fallo en la configuración de POUCT: {e}")
            return

        try:
            estados_posibles = [estado_inicial]
            entorno = EntornoConfort(
                estados=estados_posibles, estado_inicial=estado_inicial)
            logging.debug(f"[DEBUG] Entorno creado correctamente: {entorno}")
        except Exception as e:
            logging.error(f"[ERROR] Fallo en la creación del entorno: {e}")
            return

        if not isinstance(entorno, pomdp_py.Environment):
            logging.error(
                "[ERROR] 'entorno' no es una instancia válida de pomdp_py.Environment.")
            return

        try:
            accion = pouct.plan(entorno)
            logging.debug(
                f"[DEBUG] Acción seleccionada por el planificador: {accion}")
        except Exception as e:
            logging.error(f"[ERROR] Fallo en la planificación con POUCT: {e}")
            return

    except Exception as e:
        logging.error(
            "[ERROR] Ocurrió un error durante la prueba básica:", exc_info=True)
        traceback.print_exc()

    logging.info("[INFO] Prueba básica completada.")


class ProblemaConfortTermico(pomdp_py.POMDP):

    def __init__(self, agente, entorno):
        if not isinstance(agente, AgentePersonalizado):
            raise ValueError(
                "[ERROR] 'agente' debe ser una instancia de AgentePersonalizado.")
        if not isinstance(entorno, EntornoConfort):
            raise ValueError(
                "[ERROR] 'entorno' debe ser una instancia de EntornoConfort.")

        super().__init__(agente, entorno)
        self.agente = agente
        self.entorno = entorno

        logging.info(
            "[INFO] ProblemaConfortTermico inicializado correctamente.")

    @staticmethod
    def crear(ruido_observacion, pmv_deseado, num_particulas=1000, num_estados=50):
        try:
            logging.info(
                "[INFO] Configurando el problema de confort térmico...")

            creencia_inicial = crear_creencia_inicial_en_particulas(
                num_particulas=num_particulas)
            if not isinstance(creencia_inicial, pomdp_py.Particles):
                raise ValueError("[ERROR] La creencia inicial no es válida.")

            policy_model = ModeloDePolitica(pmv_deseado=pmv_deseado)
            transition_model = ModeloDeTransicion()
            observation_model = ModeloDeObservacion(
                ruido_observacion=ruido_observacion)
            reward_model = ModeloDeRecompensa(pmv_deseado=pmv_deseado)

            agente = AgentePersonalizado(
                nombre="AgenteConfortTermico",
                pmv=pmv_deseado,
                belief=creencia_inicial,
                policy=policy_model,
                transition_model=transition_model,
                observation_model=observation_model,
                reward_model=reward_model
            )
            logging.info("[INFO] Agente configurado con éxito.")

            estados_posibles = [
                EstadoConfortTermico.generar_estado_aleatorio() for _ in range(num_estados)]
            estado_inicial = estados_posibles[np.random.randint(
                0, len(estados_posibles))]

            entorno = EntornoConfort(
                estados=estados_posibles, estado_inicial=estado_inicial)
            logging.info("[INFO] Entorno configurado con éxito.")

            problema = ProblemaConfortTermico(agente, entorno)
            logging.info("[INFO] ProblemaConfortTermico creado con éxito.")

            return problema

        except Exception as e:
            logging.error(
                "[ERROR] Ocurrió un error durante la creación del problema:", exc_info=True)
            traceback.print_exc()
            raise e


def safe_plan_call(planificador, agente):

    logging.info(
        "[DEBUG] Verificando agente y planificador antes de ejecutar plan()...")

    if not hasattr(agente, "cur_belief") or not hasattr(agente, "policy"):
        logging.error(
            "[ERROR] El objeto agente no contiene las propiedades necesarias (cur_belief, policy).")
        return AccionConfortTermico(cambio_de_temperatura=0, cambio_de_flujo_de_aire=0)

    logging.debug(f"[DEBUG] Tipo de creencia del agente: {
                  type(agente.cur_belief)}")

    if not agente.cur_belief.particles:
        logging.warning(
            "[WARNING] `belief.particles` está vacío, generando partículas de respaldo...")
        agente.set_belief(pomdp_py.Particles([
            Particula(EstadoConfortTermico.generar_estado_aleatorio(), weight=1.0)
        ]))

    particulas_seguras = [
        p for p in agente.cur_belief.particles if isinstance(p, Particula)]

    if not particulas_seguras:
        logging.warning(
            "[WARNING] No hay partículas válidas después del filtrado. Se generará una de respaldo.")
        particulas_seguras.append(
            Particula(EstadoConfortTermico.generar_estado_aleatorio(), weight=1.0))

    peso_total = sum(p.weight for p in particulas_seguras) or 1.0
    for p in particulas_seguras:
        p.weight = max(0.01, min(p.weight / peso_total, 1.5))

    state_list = convert_belief_to_state_list(
        pomdp_py.Particles(particulas_seguras))
    agente.set_belief(pomdp_py.Particles(copy.deepcopy(state_list)))
    agente.cur_belief_states = copy.deepcopy(state_list)

    logging.debug(f"[DEBUG] Tamaño de belief.particles después de asignación: {
                  len(agente.cur_belief.particles)}")

    for idx, s in enumerate(agente.cur_belief.particles[:5]):
        logging.info(f"[DEBUG] Partícula {idx}: Tipo: {
                     type(s)}, contenido: {s}")

    logging.debug(f"[DEBUG] ¿Planificador POUCT tiene método de búsqueda?: {
                  'plan' in dir(planificador)}")

    try:
        resultado = planificador.plan(agente)
        if resultado is None:
            raise ValueError("[ERROR] El planificador devolvió None.")
    except Exception as e:
        logging.warning(f"[WARNING] Se produjo una excepción en `plan()`: {e}")
        return AccionConfortTermico(cambio_de_temperatura=0, cambio_de_flujo_de_aire=0)

    logging.info(f"[DEBUG] Acción planificada con éxito: {resultado}")
    return resultado


def probar_planificador_con_arbol(problema_confort, planificador, pasos=5):

    logging.info(f"[DEBUG] Probando planificador {
                 type(planificador).__name__} con árbol de búsqueda...")

    tiene_arbol = hasattr(
        planificador, 'tree') and planificador.tree is not None
    logging.info(f"[DEBUG] ¿Planificador tiene árbol?: {tiene_arbol}")

    if tiene_arbol:
        try:
            logging.debug(
                "[DEBUG] ** Inspeccionando el árbol con TreeDebugger **")
            debugger = TreeDebugger(planificador.tree)
            debugger.display(width=80)
        except Exception as e:
            logging.error(
                f"[ERROR] Fallo al inspeccionar el árbol con TreeDebugger: {e}")

    accion = safe_plan_call(planificador, problema_confort.agente)

    for i in tqdm(range(pasos), desc="Ejecutando simulación"):
        logging.info(f"\n[DEBUG] === Paso {i+1} ===")

        try:
            observacion_real = problema_confort.agente.observation_model.sample(
                problema_confort.entorno.estado_inicial, accion
            )

            update_belief_with_resample(
                problema_confort.agente, accion, observacion_real)

            planificador.update(problema_confort.agente,
                                accion, observacion_real)

        except Exception as e:
            logging.error(
                f"[ERROR] Fallo en la actualización durante la simulación: {e}")

        accion = safe_plan_call(planificador, problema_confort.agente)

    logging.info("[INFO] Simulación de planificación con árbol completada.")


def main():

    logging.info(
        "[INFO] Iniciando la simulación del problema de confort térmico.")

    try:
        confort = ProblemaConfortTermico.crear(
            ruido_observacion=0.15, pmv_deseado=0.0)

        if not isinstance(confort, ProblemaConfortTermico):
            raise ValueError(
                "[ERROR] No se generó una instancia válida de ProblemaConfortTermico.")

        if not hasattr(confort, "agente") or not hasattr(confort.agente, "policy"):
            raise AttributeError(
                "[ERROR] El problema generado no tiene un agente válido o su política.")

    except Exception as e:
        logging.error(
            "[ERROR] Ocurrió un error al crear el problema de confort térmico:", exc_info=True)
        traceback.print_exc()
        return

    if hasattr(confort.agente, 'get_belief'):
        belief_inicial = confort.agente.get_belief()
        logging.info(f"[DEBUG] Belief inicial: {belief_inicial}")

    try:
        planificadores = {
            "POUCT": pomdp_py.POUCT(
                max_depth=20,
                discount_factor=0.95,
                num_sims=5000,
                exploration_const=1.2,
                rollout_policy=confort.agente.policy
            ),
            "POMCP": pomdp_py.POMCP(
                max_depth=20,
                discount_factor=0.95,
                num_sims=5000,
                exploration_const=1.2,
                rollout_policy=confort.agente.policy
            )
        }

    except Exception as e:
        logging.error(
            "[ERROR] Falló la inicialización de los planificadores:", exc_info=True)
        return

    for nombre, planificador in planificadores.items():
        try:
            logging.info(
                f"\n[INFO] ** Prueba de {nombre} con análisis de árbol **")
            logging.info(f"[DEBUG] Iniciando prueba para {nombre}")

            tiene_arbol = hasattr(
                planificador, 'tree') and planificador.tree is not None
            logging.info(f"[DEBUG] ¿Planificador {
                         nombre} tiene árbol?: {tiene_arbol}")

            if hasattr(confort.agente, 'get_belief'):
                belief_antes = confort.agente.get_belief()
                logging.debug(
                    f"[{nombre}] Belief antes de planificar: {belief_antes}")

            probar_planificador_con_arbol(confort, planificador, pasos=5)

            if hasattr(confort.agente, 'get_belief'):
                belief_despues = confort.agente.get_belief()
                logging.debug(f"[{nombre}] Belief después de planificar: {
                              belief_despues}")

        except Exception as e:
            logging.error(f"[ERROR] Falló la prueba del planificador {
                          nombre}:", exc_info=True)

    logging.info("[INFO] Simulación finalizada con éxito.")


if __name__ == "__main__":
    logging.info("[INFO] Ejecutando la simulación principal...")
    logging.basicConfig(level=logging.DEBUG)
    main()
